import operator
import os
import inspect
from typing import Any, Dict, List, Tuple, Callable

import numpy as np
import torch
import torch.nn
import torch.fx
import jinja2
import subprocess

from MiCoQLayers import BitConv2d, BitLinear, weight_quant
from MiCoUtils import weight_export, fuse_model, fuse_model_seq, get_model_macs
from MiCoRegistry import MiCoOpRegistry

from models.LLaMa import TransformerBlock
'''
Modified Trace Module to Support BitConv2d and BitLinear
'''
class MiCoTrace(torch.fx.Tracer):        
    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:
        return (
            ((m.__module__.startswith("torch.nn") or m.__module__.startswith("torch.ao.nn"))
            or (isinstance(m, BitLinear) or isinstance(m, BitConv2d))
            or (hasattr(m, "MiCo_func")))
            and not isinstance(m, torch.nn.Sequential)
        )

'''
MiCo Code Generator Class
'''
class MiCoCodeGen(torch.fx.Interpreter):
    """
    This class converts a PyTorch model to a C model.
    """

    MODEL_H_TEMPLATE = """#ifndef __MODEL_H
#define __MODEL_H
/*
    Model header generated by MiCoCodeGen.
*/
#define {model_dataset}
#include "nn.h"
#include "mico_nn.h"
#include "profile.h"

// load the weight data block from the {model_name}.bin file
INCLUDE_FILE(".rodata", "./{model_name}.bin", model_weight);
extern uint8_t model_weight_data[];
extern size_t model_weight_start[];
extern size_t model_weight_end[];

// Profiler Timer
extern long QMATMUL_TIMER, QUANT_TIMER, IM2COL_TIMER;

typedef struct {{
{model_struct}
}} Model;

void model_init(Model* model) {{
{model_init}
}}

void model_forward(Model* model) {{
{model_forward}
}}

#endif  // __MODEL_H
"""
    @staticmethod
    def get_dtype_str(dtype: torch.dtype) -> str:
        """
        Convert a PyTorch dtype to a string representing the NN type.

        Args:
            dtype (torch.dtype): The PyTorch dtype to convert.
        
        Returns:
            str: The string representing the NN type.
        """
        if dtype == torch.float16:
            return "F16"
        elif dtype == torch.float32:
            return "F32"
        else:
            raise ValueError(f"Unsupported dtype: {dtype}")
        
    @staticmethod
    def get_ctype_str(dtype: torch.dtype) -> str:
        """
        Convert a PyTorch dtype to a string representing the C type.

        Args:
            dtype (torch.dtype): The PyTorch dtype to convert.
        
        Returns:
            str: The string representing the C type.
        """
        if dtype == torch.float16:
            return "float16_t"
        elif dtype == torch.float32:
            return "float32"
        else:
            raise ValueError(f"Unsupported dtype: {dtype}")

    @staticmethod
    def _extract_graph_module(model: torch.nn.Module) -> tuple[torch.fx.Graph, torch.fx.GraphModule]:
        """
        Helper function to extract the graph module from the model.

        Args:
            model (torch.nn.Module): The model to extract the graph module from.
        
        Returns:
            tuple[torch.fx.Graph, torch.fx.GraphModule]: The graph and the graph module.
        """
        graph = MiCoTrace().trace(model)
        # Does some checks to make sure the Graph is well-formed.
        graph.lint()
        gm = torch.fx.GraphModule(model, graph)
        return graph, gm

    def __init__(self, model: torch.nn.Module, align_to: int = 32):
        graph, gm = MiCoCodeGen._extract_graph_module(model)
        super().__init__(gm)

        # store the model, graph, and graph module as class attributes
        self.model: torch.nn.Module = model
        self.graph: torch.fx.Graph = graph
        self.gm: torch.fx.GraphModule = gm

        # extract node information
        self.node_info: Dict[str, Tuple[Any, Any]] = {n.name: (n.args, n.kwargs) for n in self.graph.nodes}

        # initialize jinja2 code generation environment
        self.env = jinja2.Environment()
        self.align_to = align_to

        self.reset()
    
    def reset(self):
        # arrays to hold the to-be-generated code
        self.model_struct = []
        self.model_init = []
        self.model_forward = []
        self.weight_content = b""

        # dictionaries to hold the tensor data
        self.tensors = {}

        # this is sooooo hacky
        self.placeholder_counter: Dict[str, int] = {}
        self.function_counter: Dict[str, int] = {}

    def print_graph(self):
        """
        Print the graph in a tabular format in the terminal.
        """
        self.gm.graph.print_tabular()

    def _get_inner_module(self, module: torch.nn.Module, target_hierarchy: List[str]) -> torch.nn.Module:
        """
        Get a module in a nn.Sequential layer.
        This function will recursively unpack the nn.Sequential layers to get the innermost module.
        
        Args:
            module (torch.nn.Sequential): A nn.Sequential layer.
            indicies (List[int]): A list of indicies of the layers in the nn.Sequential layer.
        
        Returns:
            The innermost module.
        """
        module_name = target_hierarchy[0]
        target_hierarchy = target_hierarchy[1:]
        submodule = getattr(module, module_name)

        if len(target_hierarchy) == 0:
            return submodule
        
        return self._get_inner_module(submodule, target_hierarchy)

    def get_module(self, module_name: str) -> torch.nn.Module:
        """
        Finds the module specified by the module name from the model.
        If the module name contains a dot, it will recursively unpack the nn.Sequential layers to get the innermost module.
        
        Args:
            module_name (str): The name of the module to get.
        
        Returns:
            The target module.
        """
        if "." in module_name:
            # if we have nn.Sequential layers
            target_hierarchy = module_name.split(".")
            return self._get_inner_module(self.model, target_hierarchy)
        
        return getattr(self.model, module_name)

    def add_uninitialized_tensor(self, name: str, tensor: torch.Tensor, quant = 0):
        """
        Add an uninitialized tensor to the C code.
        """
        self.tensors[name] = {
            "tensor": tensor,
            "initialized": False,
            "quantized" : quant
        }
    
    def add_initialized_tensor(self, name: str, tensor: torch.Tensor, quant = 0, scale = 0.0):
        """
        Add an initialized tensor to the C code.
        """
        self.tensors[name] = {
            "tensor": tensor,
            "initialized": True,
            "quantized" : quant,
            "scale" : scale,
            "bypass": False
        }
    
    def add_connect_tensor(self, name: str, tensor: torch.Tensor, quant = 0):
        self.tensors[name] = {
            "tensor": tensor,
            "initialized": True,
            "quantized" : quant,
            "bypass": True
        }

    def add_forward_call(self, function_name: str, out: torch.Tensor, layer_name: str, input_names: List[str], parameters: List[str] = None):
        """
        This method creates the C code for the forward call.

        Args:
            function (Callable): The function to call.
            dim (int): The dimension of the output tensor.
            dtype (torch.dtype): The data type of the output tensor.
            layer_name (str): The name of the layer.
            input_names (List[str]): The names of the input tensors.
        """
        
        dtype_str = MiCoCodeGen.get_dtype_str(out.dtype)
        
        # get the nn function name and format it
        function_name = function_name.format(
            dim=out.dim(),
            dtype=dtype_str.lower()
            )
        
        # get the argument list
        args_list = [layer_name] + input_names  # output tensor is the same as layer name
        args_list = [f"&model->{arg_name}" for arg_name in args_list]
        if parameters:
            args_list += [str(param) for param in parameters]
        arg_list_str = ", ".join(args_list)
    
        self.model_forward.append(f"{function_name}({arg_list_str});")
    
    def _extract_input_names(self, n: torch.fx.node.Node) -> List[str]:
        """
        Extract input tensor names from a node.
        
        Args:
            n: The FX node to extract input names from.
            
        Returns:
            List of input tensor names.
        """
        input_names = []
        for node in self.node_info[n.name][0]:
            # Skip constant values (int, float, bool, str, None)
            if isinstance(node, (int, float, bool, str, type(None))):
                pass
            elif type(node) is torch.fx.immutable_collections.immutable_list:
                input_names += [i.name for i in node]
            elif hasattr(node, 'name'):
                input_names.append(node.name)
        return input_names
    

    def handle_placeholder(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("placeholder:", n.name)
        self.add_uninitialized_tensor(n.name, out)

    def handle_get_attr(self, n: torch.fx.node.Node, out: torch.Tensor):
        # print("get attr:", n.name, n.target)
        pass

    def handle_call_function(self, n: torch.fx.node.Node, out: torch.Tensor):
        """
        Handle the case where the node is a call to a torch function (e.g. relu, elu, etc.)
        Uses the registry pattern to look up handlers for operations.
        """
        print("call function:", n.name, n.target, n.args)

        # get all the related information
        function = n.target
        input_names = self._extract_input_names(n)
        input_args = n.args
        
        # Try to get a registered handler for this function
        handler = MiCoOpRegistry.get_function_handler(function)
        if handler:
            handler(self, n, out, input_names, input_args)
        else:
            func_name = getattr(function, '__name__', str(function))
            func_module = getattr(function, '__module__', '')
            func_full_name = f"{func_module}.{func_name}" if func_module else func_name
            raise NotImplementedError(
                f"Function '{func_full_name}' is not registered. "
                f"Use @MiCoOpRegistry.register_function() decorator to add support."
            )
        
    def handle_call_method(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("call method:", n.name, n.target)
        method = n.target
        if method == "size":
            self.add_connect_tensor(n.name, out)
            self.add_forward_call("MiCo_CONNECT", out, n.name, [n.name])
        elif method == "view":
            self.add_connect_tensor(n.name, out)
            self.add_forward_call("MiCo_CONNECT", out, n.name, [n.name])
        else:
            raise NotImplementedError()

    def handle_call_module(self, n: torch.fx.node.Node, out: torch.Tensor):
        """
        Handle the case where the node is a call to a torch module.
        Uses the registry pattern to look up handlers for operations.
        """
        print("call module:", n.name, n.target)

        module = self.get_module(n.target)
        layer_name = n.name
        input_names = [n.name for n in self.node_info[n.name][0]]

        # Try to get a registered handler for this module type
        handler = MiCoOpRegistry.get_module_handler(module)
        if handler:
            handler(self, n, out, module, input_names)
        # Check for custom MiCo_func attribute as fallback
        elif hasattr(module, "MiCo_func"):
            input_names += module.MiCo_func.input_names
            parameters = module.MiCo_func.params
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call(module.MiCo_func.name, out, layer_name, input_names, parameters)
        else:
            module_type = type(module)
            module_full_name = f"{module_type.__module__}.{module_type.__name__}"
            raise NotImplementedError(
                f"Module '{module_full_name}' is not registered. "
                f"Use @MiCoOpRegistry.register_module() decorator with the module class to add support."
            )


    def handle_output(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("output:", n.name, out.shape, out.dtype)
        n_size = out.nelement() * out.element_size()
        
        self.add_uninitialized_tensor(n.name, out)
        self.model_forward.append(f"memcpy(model->output.data, model->{n.args[0].name}.data, {n_size});")
        
    def run_node(self, n: torch.fx.node.Node) -> torch.Tensor:
        out = super().run_node(n)

        if n.op == "placeholder":
            self.handle_placeholder(n, out)
        elif n.op == "call_module":
            self.handle_call_module(n, out)
        elif n.op == "get_attr":
            self.handle_get_attr(n, out)
        elif n.op == "call_function":
            self.handle_call_function(n, out)
        elif n.op == "call_method":
            self.handle_call_method(n, out)
        elif n.op == "output":
            self.handle_output(n, out)

        return out

    def convert(self, output_directory: str = "project", 
                model_name: str = "model", verbose = False):
        """
        Convert the model to a C model.

        Args:
            args: The input to the model.
            kwargs: The keyword arguments to the model.
        
        Returns:
            The output of the model.
        """
        
        align_to = self.align_to

        if self.example_inputs is None:
            raise ValueError("No example inputs provided. Please call forward() at least once.")

        # === Compute memory pools for optimized allocation ===
        memory_pools, tensor_to_pool = self.allocate_memory_pools()
        
        # Calculate total memory savings
        total_without_pooling = 0
        total_with_pooling = 0
        for name, tensor_dict in self.tensors.items():
            if not tensor_dict.get("initialized", False) and tensor_dict["tensor"] is not None:
                total_without_pooling += tensor_dict["tensor"].nelement() * tensor_dict["tensor"].element_size()
        for pool in memory_pools:
            total_with_pooling += pool['size']
        
        print(f"Memory optimization: {total_without_pooling} bytes -> {total_with_pooling} bytes")
        print(f"Memory saved: {total_without_pooling - total_with_pooling} bytes ({100.0 * (total_without_pooling - total_with_pooling) / total_without_pooling:.1f}%)")
        print(f"Number of memory pools: {len(memory_pools)}")
        
        # === Add memory pool declarations to model struct ===
        for pool_id, pool in enumerate(memory_pools):
            self.model_struct.append(f"float *memory_pool_{pool_id};")
        
        # === Allocate memory pools in model_init ===
        for pool_id, pool in enumerate(memory_pools):
            self.model_init.append(f"model->memory_pool_{pool_id} = (float *)malloc({pool['size']});")

        # === Generate the tensor structs and initialize routines for the tensors in the C code. ===
        for name, tensor_dict in self.tensors.items():
            initialized = tensor_dict["initialized"]
            qbit = tensor_dict["quantized"]
            tensor = tensor_dict["tensor"]

            if tensor is not None:
                dim = tensor.dim()
                if qbit == 0:
                    dtype_str = MiCoCodeGen.get_dtype_str(tensor.dtype)
                else:
                    # dtype_str = f"Q{qbit}"
                    dtype_str = "Q8"
                self.model_struct.append(f"Tensor{dim}D_{dtype_str} {name};")

                for i in range(dim):
                    self.model_init.append(f"model->{name}.shape[{i}] = {tensor.shape[i]};")

                if initialized:
                    if tensor_dict["bypass"]:
                        continue
                    if qbit == 0:
                        self.model_init.append(f"model->{name}.data = (float *)(model_weight_data + {len(self.weight_content)});")    
                        self.weight_content += tensor.detach().numpy().tobytes()
                        # If align > 32, we need to align the weight data to the specified alignment
                        # Currently only consider 64-bit alignment
                        if len(self.weight_content) % (align_to // 8) != 0:
                            self.weight_content += b'\x00' * (len(self.weight_content) % (align_to // 8))
                    else:
                        qweight, scale = weight_quant(tensor, qbit)
                        self.model_init.append(f"model->{name}.data = (qbyte *)(model_weight_data + {len(self.weight_content)});")
                        self.weight_content += weight_export(qweight, qbit, align_to)
                        self.model_init.append(f"model->{name}.scale = {scale};")
                else:
                    # Use memory pool instead of individual malloc
                    if name in tensor_to_pool:
                        pool_id, offset = tensor_to_pool[name]
                        self.model_init.append(f"model->{name}.data = model->memory_pool_{pool_id};")
                    else:
                        # Fallback to malloc if not in any pool (shouldn't happen normally)
                        n_size = tensor.nelement() * tensor.element_size()
                        self.model_init.append(f"model->{name}.data = (float *)malloc({n_size});")
            else:
                dim = 1
                dtype_str = "F32"
                self.model_struct.append(f"Tensor{dim}D_{dtype_str} {name};")
                self.model_init.append(f"model->{name}.shape[0] = 0;")

        print("finished tracing the model")

        # === Write the generated C code to the output directory. ===
        # create the output directory if it doesn't exist
        os.makedirs(output_directory, exist_ok=True)

        INDENT = "    "
        model_struct = [f"{INDENT}{line}" for line in self.model_struct]
        model_init = [f"{INDENT}{line}" for line in self.model_init]
        model_forward = [f"{INDENT}{line}" for line in self.model_forward]
        
        # Insert Profiler
        start_profiler = [f"{INDENT}long profile_time = MiCo_time();"]
        end_profiler = [f"{INDENT}profile_time = MiCo_time() - profile_time;",
                        f"{INDENT}printf(\"Execution Time: %ld\\n\", profile_time);",
                        f"{INDENT}printf(\"QMatMul Time: %ld\\n\", QMATMUL_TIMER);",
                        f"{INDENT}printf(\"Quantization Time: %ld\\n\", QUANT_TIMER);",
                        f"{INDENT}printf(\"Im2Col Time: %ld\\n\", IM2COL_TIMER);"]
        model_forward = start_profiler + model_forward + end_profiler

        model_struct_str = "\n".join(model_struct)
        model_init_str = "\n".join(model_init)
        if verbose:
            model_forward_str = ""
            func_count = 0
            for line in model_forward:
                model_forward_str += f"{INDENT}printf(\"{func_count}:{line[len(INDENT):line.find('(')]}\\n\");\n"
                model_forward_str += f"{line}\n"
                func_count += 1
        else:
            model_forward_str = "\n".join(model_forward)

        model_h_path = os.path.join(output_directory, f"{model_name}.h")
        model_bin_path = os.path.join(output_directory, f"{model_name}.bin")

        if hasattr(self.model, "default_dataset"):
            model_dataset = self.model.default_dataset
        else:
            model_dataset = "DEFAULT_DATASET"

        with open(model_h_path, "w") as f:
            f.write(MiCoCodeGen.MODEL_H_TEMPLATE.format(
                model_dataset=model_dataset,
                model_name=model_name,
                model_struct=model_struct_str,
                model_init=model_init_str,
                model_forward=model_forward_str
            ))
        
        with open(model_bin_path, "wb") as f:
            f.write(self.weight_content)
        
        print(f"wrote the model to {model_h_path} and {model_bin_path}")
        print(f"model size = {len(self.weight_content)} bytes")

    def build(self, build_dir: str = "project", target: str = "mico", options: str = ""):
        """
        Build the model using the provided build directory.
        """
        # check if the build directory exists
        if not os.path.exists(build_dir):
            raise FileNotFoundError(f"Build directory {build_dir} does not exist.")
        
        target_options = ""
        # compile the model
        if target == "mico":
            target_options += "TARGET=vexii OPT=simd MARCH=rv32imc"
        if target == "mico_fpu":
            target_options += "TARGET=vexii OPT=simd MARCH=rv32imfc"
        elif target == "vexii":
            target_options += "TARGET=vexii MARCH=rv32imc"
        elif target == "vexii_fpu":
            target_options += "TARGET=vexii MARCH=rv32imfc"
        elif target == "host":
            target_options += ""
        target_options += " " + options
        cmd = f"cd {build_dir} && make clean && make -j " + target_options
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        proc.wait()
        if proc.stderr:
            print("Error in compiling the model:")
            print(proc.stderr.readlines())
            return None
        else:
            print("Model compiled successfully!")
        return proc.returncode
    
    def extract_tensor_dag(self):
        """
        Extract the tensor dependencies from the graph as a Directed Acyclic Graph.
        
        Returns:
            dict: A dictionary where keys are node names and values are lists of nodes they depend on.
        """
        dag = {}
        
        # Iterate through all nodes in the graph
        for node in self.graph.nodes:
            if node.op == 'output':
                continue  # Skip output nodes for the dependency collection
            
            # For this node, find all input tensors it depends on
            dependencies = []
            
            if node.op in ['call_module', 'call_function', 'call_method']:
                # Get arguments to the call
                for arg in self.node_info[node.name][0]:
                    if isinstance(arg, torch.fx.node.Node):
                        dependencies.append(arg.name)
                    elif isinstance(arg, (list, tuple)) or hasattr(arg, '__iter__') and not isinstance(arg, str):
                        for item in arg:
                            if isinstance(item, torch.fx.node.Node):
                                dependencies.append(item.name)
            
            # Store the dependencies for this node
            dag[node.name] = dependencies
        
        return dag
    
    def node_is_bit_op(self, node):
        is_bit_op = False
        for n in self.graph.nodes:
            if n.name == node:
                if n.op == 'call_module':
                    module = self.get_module(n.target)
                    is_bit_op = isinstance(module, (BitConv2d, BitLinear))
                    break
        return is_bit_op
    
    def extract_simplified_dag(self):
        """
        Extract a simplified tensor DAG where non-quantized operations are aggregated
        while BitConv2d and BitLinear operations are preserved.
        
        Args:
            keep_bit_ops_only (bool): If True, only keep BitConv2d and BitLinear operations.
            
        Returns:
            dict: A simplified DAG dictionary.
        """
        # Get the full DAG first
        full_dag = self.extract_tensor_dag()
                
        # Create simplified DAG
        simplified_dag = {}

        for node, dependencies in full_dag.items():
            if self.node_is_bit_op(node):
                # If the node is a BitConv2d or BitLinear operation, keep it
                simplified_dag[node] = []
                deps = dependencies.copy()
                while len(deps) > 0:
                    dep = deps.pop(0)
                    if self.node_is_bit_op(dep):
                        simplified_dag[node].append(dep)
                    else:
                        deps += full_dag[dep]

        return simplified_dag
    
    def visualize_dag(self, output_file="model_dag.png", simplified=False):
        """
        Visualize the tensor dependency DAG using graphviz.
        
        Args:
            output_file (str): The output file to save the visualization.
            simplified (bool): If True, use the simplified DAG.
            bit_ops_only (bool): If True, only show BitConv2d and BitLinear operations.
            
        Returns:
            bool: True if successful, False otherwise.
        """
        try:
            import graphviz
        except ImportError:
            print("Graphviz not found. Please install graphviz with 'pip install graphviz'")
            return False
        
        # Get the appropriate DAG
        if simplified:
            dag = self.extract_simplified_dag()
        else:
            dag = self.extract_tensor_dag()
        
        # Create a new graph
        title = 'Model Tensor Dependencies'
        if simplified:
            title += ' (Simplified)'
            
        dot = graphviz.Digraph(comment=title)
        
        # Add nodes
        for node in dag:
            # Get node type if available
            node_type = "Unknown"
            node_bits = ""
            is_bit_op = False
            for n in self.graph.nodes:
                if n.name == node:
                    if n.op == 'call_module':
                        module = self.get_module(n.target)
                        node_type = type(module).__name__
                        is_bit_op = isinstance(module, (BitConv2d, BitLinear))
                    elif n.op == 'call_function':
                        node_type = n.target.__name__ if hasattr(n.target, '__name__') else str(n.target)
                    else:
                        node_type = n.op
                    break
            
            # Use different colors for BitConv2d/BitLinear nodes
            attrs = {}
            if is_bit_op:
                attrs['color'] = 'red'
                attrs['style'] = 'filled'
                attrs['fillcolor'] = 'lightpink'

                node_bits = f"W{module.qtype}A{module.act_q}"

            node_info = f"{node}\n({node_type})"
            if is_bit_op:
                node_info += f"({node_bits})"
            # Add node with type info
            dot.node(node, node_info, **attrs)
        
        # Add edges
        for node, dependencies in dag.items():
            for dep in dependencies:
                if dep in dag:  # Only add edge if both nodes are in the DAG
                    dot.edge(dep, node)
        
        # Render the graph
        try:
            dot.render(output_file.rsplit('.', 1)[0], format=output_file.rsplit('.', 1)[1], cleanup=True)
            print(f"DAG visualization saved to {output_file}")
            return True
        except Exception as e:
            print(f"Error rendering graph: {e}")
            return False

    
    def compute_tensor_lifetimes(self):
        """
        Compute the lifetime of each tensor in the computation graph.
        
        Returns:
            dict: A dictionary mapping tensor names to (first_use, last_use) tuples,
                  where use is represented by the node index in execution order.
        """
        dag = self.extract_tensor_dag()
        
        # Get execution order of nodes
        node_order = {}
        idx = 0
        output_node_idx = None
        for node in self.graph.nodes:
            if node.op == 'output':
                # For output nodes, record which tensors are outputs
                output_node_idx = idx
                # Also add the output tensor itself if it exists in self.tensors
                if node.name in self.tensors:
                    node_order[node.name] = idx
                for arg in node.args:
                    if isinstance(arg, torch.fx.node.Node):
                        if arg.name not in node_order:
                            node_order[arg.name] = idx
                idx += 1
                continue
            node_order[node.name] = idx
            idx += 1
        
        # Track when each tensor is first created and last used
        tensor_lifetimes = {}
        
        # Initialize lifetimes: first_use is when tensor is created
        for node_name, node_idx in node_order.items():
            if node_name not in self.tensors:
                continue
            tensor_info = self.tensors[node_name]
            # Skip initialized tensors (weights) as they're used throughout
            if tensor_info.get("initialized", False) and not tensor_info.get("bypass", False):
                continue
            tensor_lifetimes[node_name] = [node_idx, node_idx]
        
        # Update last_use by checking all dependencies
        for node_name in node_order:
            if node_name not in dag:
                continue
            node_idx = node_order[node_name]
            for dep_name in dag[node_name]:
                if dep_name in tensor_lifetimes:
                    # Update the last use of the dependency
                    tensor_lifetimes[dep_name][1] = max(tensor_lifetimes[dep_name][1], node_idx)
        
        # Handle output tensors - they must remain alive until the end
        if output_node_idx is not None:
            for node in self.graph.nodes:
                if node.op == 'output':
                    for arg in node.args:
                        if isinstance(arg, torch.fx.node.Node) and arg.name in tensor_lifetimes:
                            tensor_lifetimes[arg.name][1] = output_node_idx  # Keep until output
        
        return tensor_lifetimes
    
    def allocate_memory_pools(self):
        """
        Allocate memory pools for tensors by reusing buffers for non-overlapping lifetimes.
        
        Returns:
            tuple: (memory_pools, tensor_to_pool) where:
                - memory_pools is a list of (size, tensors_in_pool) tuples
                - tensor_to_pool maps tensor names to (pool_id, offset_in_pool)
        """
        lifetimes = self.compute_tensor_lifetimes()
        
        # Collect uninitialized tensors that need memory allocation
        tensors_to_allocate = []
        for name, tensor_info in self.tensors.items():
            # Only allocate memory for uninitialized tensors (activations/intermediates)
            if not tensor_info.get("initialized", False):
                if tensor_info["tensor"] is not None:
                    size = tensor_info["tensor"].nelement() * tensor_info["tensor"].element_size()
                    if name in lifetimes:
                        tensors_to_allocate.append((name, size, lifetimes[name][0], lifetimes[name][1]))
        
        # Sort tensors by size (descending) for better packing
        tensors_to_allocate.sort(key=lambda x: x[1], reverse=True)
        
        # Memory pools: each pool is a dict with 'size', 'tensors', and 'intervals'
        memory_pools = []
        tensor_to_pool = {}  # Maps tensor_name -> (pool_id, offset)
        
        # Greedy allocation: for each tensor, find or create a pool
        for tensor_name, tensor_size, first_use, last_use in tensors_to_allocate:
            allocated = False
            
            # Try to find an existing pool where this tensor can fit
            for pool_id, pool in enumerate(memory_pools):
                # Check if this tensor's lifetime overlaps with any tensor in this pool
                can_reuse = True
                for existing_tensor, existing_first, existing_last in pool['intervals']:
                    # Check for overlap: [first_use, last_use] overlaps with [existing_first, existing_last]
                    if not (last_use < existing_first or first_use > existing_last):
                        can_reuse = False
                        break
                
                if can_reuse:
                    # We can reuse this pool
                    # Update pool size if needed
                    pool['size'] = max(pool['size'], tensor_size)
                    pool['tensors'].append(tensor_name)
                    pool['intervals'].append((tensor_name, first_use, last_use))
                    tensor_to_pool[tensor_name] = (pool_id, 0)  # offset is always 0 for reused pools
                    allocated = True
                    break
            
            # If no suitable pool found, create a new one
            if not allocated:
                pool_id = len(memory_pools)
                memory_pools.append({
                    'size': tensor_size,
                    'tensors': [tensor_name],
                    'intervals': [(tensor_name, first_use, last_use)]
                })
                tensor_to_pool[tensor_name] = (pool_id, 0)
        
        return memory_pools, tensor_to_pool


    def forward(self, *args):
        self.reset()
        self.example_inputs = args

        output = self.run(*args)

        return output
    
    def __call__(self, *args):
        return self.forward(*args)

if __name__ == "__main__":
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import MiCoUtils as mico
    from models import MLP, LeNet, CmsisCNN, VGG, SqueezeNet, MobileNetV2, \
          resnet_alt_8, resnet_alt_18, shufflenet

    torch.manual_seed(0)

    # example_input = torch.randn(1, 256) # MNIST Flatten
    example_input = torch.randn(1, 1, 28, 28) # MNIST 28x28
    # example_input = torch.randn(1, 3, 32, 32) # CIFAR-10/100

    # m = MLP(in_features=256, config={"Layers": [64, 64, 64, 10]})
    # ckpt = torch.load("output/ckpt/mlp_mnist_mp.pth")

    # m = MLP(in_features=256, config={"Layers": [61, 53, 31, 10]})
    # ckpt = torch.load("output/ckpt/mlp_mnist_misalign.pth")

    m = LeNet(1)
    ckpt = torch.load("output/ckpt/lenet_mnist.pth")

    # m = CmsisCNN(in_channels=3)
    # ckpt = torch.load("output/ckpt/cmsiscnn_cifar10_mp.pth")

    # m = VGG(in_channels=3, num_class=10)
    # ckpt = torch.load("output/ckpt/vgg_cifar10.pth")

    # m = MobileNetV2(10)
    # ckpt = torch.load("output/ckpt/mobilenetv2_cifar10.pth")
    # m.default_dataset = "CIFAR10"

    # m = SqueezeNet(class_num=10)
    # ckpt = torch.load("output/ckpt/squeeze_cifar10.pth")
    # m.default_dataset = "CIFAR10"

    # m = shufflenet(10)
    # m.default_dataset = "CIFAR10"
    # ckpt = torch.load("output/ckpt/shuffle_cifar10.pth")

    # m = resnet_alt_8(10)
    # m.default_dataset = "CIFAR10"
    # ckpt = torch.load("output/ckpt/resnet8_cifar10.pth")

    # m = resnet_alt_18(100)
    # ckpt = torch.load("output/ckpt/resnet18_cifar100.pth", map_location="cpu")

    weight_q = [8] * m.n_layers
    activation_q = [8] * m.n_layers

    m.load_state_dict(ckpt)
    m.set_qscheme([weight_q, activation_q])
    m=fuse_model(m)
    m.eval()

    m = MiCoCodeGen(m, align_to=32)
    m.forward(example_input)
    # m.visualize_dag("model_full.png")
    # m.visualize_dag("model_simplified.png", simplified=True)
    m.convert("project", "model", verbose = True)
    # m.tensor_lifetime()
    # m.build("project", "mico_fpu", "TEST_NUM=1")
