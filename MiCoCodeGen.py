import operator
import os
import inspect
from typing import Any, Dict, List, Tuple, Callable

import numpy as np
import torch
import torch.nn
import torch.fx
import jinja2
import subprocess

from MiCoQLayers import BitConv2d, BitLinear, weight_quant
from MiCoUtils import weight_export, fuse_model, fuse_model_seq, get_model_macs

from models.LLaMa import TransformerBlock
'''
Modified Trace Module to Support BitConv2d and BitLinear
'''
class MiCoTrace(torch.fx.Tracer):        
    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:
        return (
            ((m.__module__.startswith("torch.nn") or m.__module__.startswith("torch.ao.nn"))
            or (isinstance(m, BitLinear) or isinstance(m, BitConv2d))
            or (hasattr(m, "MiCo_func")))
            and not isinstance(m, torch.nn.Sequential)
        )

'''
MiCo Code Generator Class
'''
class MiCoCodeGen(torch.fx.Interpreter):
    """
    This class converts a PyTorch model to a C model.
    """

    MODEL_H_TEMPLATE = """#ifndef __MODEL_H
#define __MODEL_H
/*
    Model header generated by MiCoCodeGen.
*/
#define {model_dataset}
#include "nn.h"
#include "mico_nn.h"
#include "profile.h"

// load the weight data block from the {model_name}.bin file
INCLUDE_FILE(".rodata", "./{model_name}.bin", model_weight);
extern uint8_t model_weight_data[];
extern size_t model_weight_start[];
extern size_t model_weight_end[];

// Profiler Timer
extern long QMATMUL_TIMER, QUANT_TIMER, IM2COL_TIMER;

typedef struct {{
{model_struct}
}} Model;

void model_init(Model* model) {{
{model_init}
}}

void model_forward(Model* model) {{
{model_forward}
}}

#endif  // __MODEL_H
"""
    @staticmethod
    def get_dtype_str(dtype: torch.dtype) -> str:
        """
        Convert a PyTorch dtype to a string representing the NN type.

        Args:
            dtype (torch.dtype): The PyTorch dtype to convert.
        
        Returns:
            str: The string representing the NN type.
        """
        if dtype == torch.float16:
            return "F16"
        elif dtype == torch.float32:
            return "F32"
        else:
            raise ValueError(f"Unsupported dtype: {dtype}")
        
    @staticmethod
    def get_ctype_str(dtype: torch.dtype) -> str:
        """
        Convert a PyTorch dtype to a string representing the C type.

        Args:
            dtype (torch.dtype): The PyTorch dtype to convert.
        
        Returns:
            str: The string representing the C type.
        """
        if dtype == torch.float16:
            return "float16_t"
        elif dtype == torch.float32:
            return "float32"
        else:
            raise ValueError(f"Unsupported dtype: {dtype}")

    @staticmethod
    def _extract_graph_module(model: torch.nn.Module) -> tuple[torch.fx.Graph, torch.fx.GraphModule]:
        """
        Helper function to extract the graph module from the model.

        Args:
            model (torch.nn.Module): The model to extract the graph module from.
        
        Returns:
            tuple[torch.fx.Graph, torch.fx.GraphModule]: The graph and the graph module.
        """
        graph = MiCoTrace().trace(model)
        # Does some checks to make sure the Graph is well-formed.
        graph.lint()
        gm = torch.fx.GraphModule(model, graph)
        return graph, gm

    def __init__(self, model: torch.nn.Module, align_to: int = 32):
        graph, gm = MiCoCodeGen._extract_graph_module(model)
        super().__init__(gm)

        # store the model, graph, and graph module as class attributes
        self.model: torch.nn.Module = model
        self.graph: torch.fx.Graph = graph
        self.gm: torch.fx.GraphModule = gm

        # extract node information
        self.node_info: Dict[str, Tuple[Any, Any]] = {n.name: (n.args, n.kwargs) for n in self.graph.nodes}

        # initialize jinja2 code generation environment
        self.env = jinja2.Environment()
        self.align_to = align_to

        self.reset()
    
    def reset(self):
        # arrays to hold the to-be-generated code
        self.model_struct = []
        self.model_init = []
        self.model_forward = []
        self.weight_content = b""

        # dictionaries to hold the tensor data
        self.tensors = {}

        # this is sooooo hacky
        self.placeholder_counter: Dict[str, int] = {}
        self.function_counter: Dict[str, int] = {}

    def print_graph(self):
        """
        Print the graph in a tabular format in the terminal.
        """
        self.gm.graph.print_tabular()

    def _get_inner_module(self, module: torch.nn.Module, target_hierarchy: List[str]) -> torch.nn.Module:
        """
        Get a module in a nn.Sequential layer.
        This function will recursively unpack the nn.Sequential layers to get the innermost module.
        
        Args:
            module (torch.nn.Sequential): A nn.Sequential layer.
            indicies (List[int]): A list of indicies of the layers in the nn.Sequential layer.
        
        Returns:
            The innermost module.
        """
        module_name = target_hierarchy[0]
        target_hierarchy = target_hierarchy[1:]
        submodule = getattr(module, module_name)

        if len(target_hierarchy) == 0:
            return submodule
        
        return self._get_inner_module(submodule, target_hierarchy)

    def get_module(self, module_name: str) -> torch.nn.Module:
        """
        Finds the module specified by the module name from the model.
        If the module name contains a dot, it will recursively unpack the nn.Sequential layers to get the innermost module.
        
        Args:
            module_name (str): The name of the module to get.
        
        Returns:
            The target module.
        """
        if "." in module_name:
            # if we have nn.Sequential layers
            target_hierarchy = module_name.split(".")
            return self._get_inner_module(self.model, target_hierarchy)
        
        return getattr(self.model, module_name)

    def add_uninitialized_tensor(self, name: str, tensor: torch.Tensor, quant = 0):
        """
        Add an uninitialized tensor to the C code.
        """
        self.tensors[name] = {
            "tensor": tensor,
            "initialized": False,
            "quantized" : quant
        }
    
    def add_initialized_tensor(self, name: str, tensor: torch.Tensor, quant = 0, scale = 0.0):
        """
        Add an initialized tensor to the C code.
        """
        self.tensors[name] = {
            "tensor": tensor,
            "initialized": True,
            "quantized" : quant,
            "scale" : scale,
            "bypass": False
        }
    
    def add_connect_tensor(self, name: str, tensor: torch.Tensor, quant = 0):
        self.tensors[name] = {
            "tensor": tensor,
            "initialized": True,
            "quantized" : quant,
            "bypass": True
        }

    def add_forward_call(self, function_name: str, out: torch.Tensor, layer_name: str, input_names: List[str], parameters: List[str] = None):
        """
        This method creates the C code for the forward call.

        Args:
            function (Callable): The function to call.
            dim (int): The dimension of the output tensor.
            dtype (torch.dtype): The data type of the output tensor.
            layer_name (str): The name of the layer.
            input_names (List[str]): The names of the input tensors.
        """
        
        dtype_str = MiCoCodeGen.get_dtype_str(out.dtype)
        
        # get the nn function name and format it
        function_name = function_name.format(
            dim=out.dim(),
            dtype=dtype_str.lower()
            )
        
        # get the argument list
        args_list = [layer_name] + input_names  # output tensor is the same as layer name
        args_list = [f"&model->{arg_name}" for arg_name in args_list]
        if parameters:
            args_list += [str(param) for param in parameters]
        arg_list_str = ", ".join(args_list)
    
        self.model_forward.append(f"{function_name}({arg_list_str});")
    

    def handle_placeholder(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("placeholder:", n.name)
        self.add_uninitialized_tensor(n.name, out)

    def handle_get_attr(self, n: torch.fx.node.Node, out: torch.Tensor):
        # print("get attr:", n.name, n.target)
        pass

    def handle_call_function(self, n: torch.fx.node.Node, out: torch.Tensor):
        """
        Handle the case where the node is a call to a torch function (e.g. relu, elu, etc.)
        """
        print("call function:", n.name, n.target, n.args)

        # get all the related information
        function = n.target
        layer_name = n.name
        input_names = []
        for node in self.node_info[n.name][0]:
            # print(n, type(n))
            if type(node) is int:
                pass
            elif type(node) is torch.fx.immutable_collections.immutable_list:
                input_names += [i.name for i in node]
            else:
                input_names.append(node.name)
        
        input_args = n.args
        
        # Math operations - Pointwise Ops
        if function == operator.__add__ or function == torch.add:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_add{dim}d_{dtype}", out, layer_name, input_names)
        
        elif function == operator.__mul__:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_mul{dim}d_{dtype}", out, layer_name, input_names)
        
        # Convolution Layers

        # Non-linear Activations
        elif function == torch.nn.functional.relu:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_relu{dim}d_{dtype}", out, layer_name, input_names)
        
        elif function == torch.nn.functional.relu6:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_relu6{dim}d_{dtype}", out, layer_name, input_names)
        
        elif function == torch.nn.functional.tanh:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_tanh{dim}d_{dtype}", out, layer_name, input_names)
        
        # Linear Layers
        elif function == torch.nn.functional.linear:
            weight = self.model.state_dict()[input_args[1].target]
            bias = self.model.state_dict()[input_args[2].target]
            self.add_uninitialized_tensor(layer_name, out)
            self.add_initialized_tensor(f"{input_names[1]}", weight)
            self.add_initialized_tensor(f"{input_names[2]}", bias)
            self.add_forward_call("MiCo_addmm_{dtype}", out, layer_name, input_names)
        
        # Pooling Functions
        elif function == torch.nn.functional.avg_pool2d:
            self.add_uninitialized_tensor(layer_name, out)
            if isinstance(input_args[1], Tuple):
                kernel_size = input_args[1][0]
            elif isinstance(input_args[1], int):
                kernel_size = input_args[1]
            if len(input_args) > 2:
                stride = input_args[2]
            else:
                stride = 1
            self.add_forward_call("MiCo_avgpool{dim}d_{dtype}", out, layer_name, input_names, 
                                  [kernel_size, stride])
        elif function == torch.nn.functional.max_pool2d:
            self.add_uninitialized_tensor(layer_name, out)
            if isinstance(input_args[1], Tuple):
                kernel_size = input_args[1][0]
            elif isinstance(input_args[1], int):
                kernel_size = input_args[1]
            if len(input_args) > 2:
                stride = input_args[2]
            else:
                stride = 1
            self.add_forward_call("MiCo_maxpool{dim}d_{dtype}", out, layer_name, input_names,
                                  [kernel_size, stride])
        elif function == torch.nn.functional.adaptive_avg_pool2d:
            self.add_uninitialized_tensor(layer_name, out)
            if isinstance(input_args[1], Tuple):
                output_size = input_args[1][0]
            elif isinstance(input_args[1], int):
                output_size = input_args[1]
            self.add_forward_call("MiCo_adaptive_avgpool{dim}d_{dtype}", out, layer_name, input_names, [output_size])

        elif function == torch.flatten:
            self.add_connect_tensor(layer_name, out)
            self.add_forward_call("MiCo_CONNECT", out, layer_name, input_names)
        
        elif function == torch.cat:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_concat{dim}d_{dtype}", out, layer_name, input_names)
        
    def handle_call_method(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("call method:", n.name, n.target)
        method = n.target
        if method == "size":
            self.add_connect_tensor(n.name, out)
            self.add_forward_call("MiCo_CONNECT", out, n.name, [n.name])
        elif method == "view":
            self.add_connect_tensor(n.name, out)
            self.add_forward_call("MiCo_CONNECT", out, n.name, [n.name])
        else:
            raise NotImplementedError()

    def handle_call_module(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("call module:", n.name, n.target)

        module = self.get_module(n.target)
        layer_name = n.name
        input_names = [n.name for n in self.node_info[n.name][0]]

        # Convolution Layers
        if type(module) is BitConv2d:
            weight = module.weight
            bias = module.bias
            input_names.append(f"{layer_name}_weight")
            input_names.append(f"{layer_name}_bias")

            self.add_uninitialized_tensor(layer_name, out)
            self.add_initialized_tensor(f"{layer_name}_weight", weight, 
                                        quant=module.qtype, scale=module.qw_scale)
            self.add_initialized_tensor(f"{layer_name}_bias", bias)

            self.add_forward_call("MiCo_bitconv2d_{dtype}", out, layer_name, input_names, [
                round(module.qtype),
                round(module.act_q),
                module.stride[0],   # assume same stride for both dimensions
                module.padding[0],  # assume same padding for both dimensions
                module.dilation[0], # assume same dilation for both dimensions
                module.groups,
                self.align_to
            ])
        elif type(module) is BitLinear:
            weight = module.weight
            bias = module.bias
            input_names.append(f"{layer_name}_weight")
            input_names.append(f"{layer_name}_bias")

            self.add_uninitialized_tensor(layer_name, out)
            self.add_initialized_tensor(f"{layer_name}_weight", weight, 
                                        quant=module.qtype, scale=module.qw_scale)
            self.add_initialized_tensor(f"{layer_name}_bias", bias)

            self.add_forward_call("MiCo_bitlinear_{dtype}", out, layer_name, input_names, [
                round(module.qtype),
                round(module.act_q),
                self.align_to])

        elif type(module) is torch.nn.Conv2d:
            weight = module.weight
            bias = module.bias
            input_names.append(f"{layer_name}_weight")
            input_names.append(f"{layer_name}_bias")

            self.add_uninitialized_tensor(layer_name, out)
            self.add_initialized_tensor(f"{layer_name}_weight", module.weight)
            self.add_initialized_tensor(f"{layer_name}_bias", module.bias)
            
            self.add_forward_call("MiCo_conv2d_{dtype}", out, layer_name, input_names, [
                module.stride[0],   # assume same stride for both dimensions
                module.padding[0],  # assume same padding for both dimensions
                module.dilation[0], # assume same dilation for both dimensions
                module.groups
            ])

        # Normalization Layers
        elif type(module) is torch.nn.BatchNorm2d:
            input_names.append(f"{layer_name}_weight")
            input_names.append(f"{layer_name}_bias")
            input_names.append(f"{layer_name}_running_mean")
            input_names.append(f"{layer_name}_running_var")
            self.add_uninitialized_tensor(layer_name, out)
            self.add_initialized_tensor(f"{layer_name}_weight", module.weight)
            self.add_initialized_tensor(f"{layer_name}_bias", module.bias)
            self.add_initialized_tensor(f"{layer_name}_running_mean", module.running_mean)
            self.add_initialized_tensor(f"{layer_name}_running_var", module.running_var)
            self.add_forward_call("MiCo_batchnorm2d_{dtype}", out, layer_name, input_names, [module.eps])
        
        # Non-linear Activations
        elif type(module) is torch.nn.ELU:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_elu{dim}d_{dtype}", out, layer_name, input_names, [module.alpha])
        
        elif type(module) is torch.nn.ReLU:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_relu{dim}d_{dtype}", out, layer_name, input_names)
        
        elif type(module) is torch.nn.ReLU6:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_relu6{dim}d_{dtype}", out, layer_name, input_names)
        
        elif type(module) is torch.nn.Tanh:
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_tanh{dim}d_{dtype}", out, layer_name, input_names)
        # Pooling Functions
        elif type(module) is torch.nn.AvgPool2d:
            self.add_uninitialized_tensor(layer_name, out)
            if isinstance(module.kernel_size, Tuple):
                kernel_size = module.kernel_size[0]
            elif isinstance(module.kernel_size, int):
                kernel_size = module.kernel_size
            self.add_forward_call("MiCo_avgpool{dim}d_{dtype}", out, layer_name, input_names, 
                                  [kernel_size, module.stride, module.padding])
        elif type(module) is torch.nn.MaxPool2d:
            if isinstance(module.kernel_size, Tuple):
                kernel_size = module.kernel_size[0]
            elif isinstance(module.kernel_size, int):
                kernel_size = module.kernel_size
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_maxpool{dim}d_{dtype}", out, layer_name, input_names, 
                                  [kernel_size, module.stride, module.padding])
        elif type(module) is torch.nn.AdaptiveAvgPool2d:
            if isinstance(module.output_size, Tuple):
                output_size = module.output_size[0]
            elif isinstance(module.output_size, int):
                output_size = module.output_size
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call("MiCo_adaptive_avgpool{dim}d_{dtype}", out, layer_name, input_names, [output_size])
        # Flatten Functions
        elif type(module) is torch.nn.Flatten:
            self.add_connect_tensor(layer_name, out)
            self.add_forward_call("MiCo_CONNECT", out, layer_name, input_names)
        # Linear Layers
        elif type(module) is torch.nn.Linear:
            weight = module.weight
            bias = module.bias
            input_names.append(f"{layer_name}_weight")
            input_names.append(f"{layer_name}_bias")

            self.add_uninitialized_tensor(layer_name, out)
            self.add_initialized_tensor(f"{layer_name}_weight", weight)
            self.add_initialized_tensor(f"{layer_name}_bias", bias)
            self.add_forward_call("MiCo_linear_{dtype}", out, layer_name, input_names)
        # Identity Layers
        elif isinstance(module, (torch.nn.Identity, torch.nn.Dropout)):
            self.add_connect_tensor(layer_name, out)
            self.add_forward_call("MiCo_CONNECT", out, layer_name, input_names)
        # Costom Layers with MiCo Implementation
        elif hasattr(module, "MiCo_func"):
            input_names += module.MiCo_func.input_names
            parameters = module.MiCo_func.params
            self.add_uninitialized_tensor(layer_name, out)
            self.add_forward_call(module.MiCo_func.name, out, layer_name, input_names, parameters)


    def handle_output(self, n: torch.fx.node.Node, out: torch.Tensor):
        print("output:", n.name, out.shape, out.dtype)
        n_size = out.nelement() * out.element_size()
        
        self.add_uninitialized_tensor(n.name, out)
        self.model_forward.append(f"memcpy(model->output.data, model->{n.args[0].name}.data, {n_size});")
        
    def run_node(self, n: torch.fx.node.Node) -> torch.Tensor:
        out = super().run_node(n)

        if n.op == "placeholder":
            self.handle_placeholder(n, out)
        elif n.op == "call_module":
            self.handle_call_module(n, out)
        elif n.op == "get_attr":
            self.handle_get_attr(n, out)
        elif n.op == "call_function":
            self.handle_call_function(n, out)
        elif n.op == "call_method":
            self.handle_call_method(n, out)
        elif n.op == "output":
            self.handle_output(n, out)

        return out

    def convert(self, output_directory: str = "project", 
                model_name: str = "model", verbose = False):
        """
        Convert the model to a C model.

        Args:
            args: The input to the model.
            kwargs: The keyword arguments to the model.
        
        Returns:
            The output of the model.
        """
        
        align_to = self.align_to

        if self.example_inputs is None:
            raise ValueError("No example inputs provided. Please call forward() at least once.")

        # === Generate the tensor structs and initialize routines for the tensors in the C code. ===
        for name, tensor_dict in self.tensors.items():
            initialized = tensor_dict["initialized"]
            qbit = tensor_dict["quantized"]
            tensor = tensor_dict["tensor"]

            if tensor is not None:
                dim = tensor.dim()
                if qbit == 0:
                    dtype_str = MiCoCodeGen.get_dtype_str(tensor.dtype)
                else:
                    # dtype_str = f"Q{qbit}"
                    dtype_str = "Q8"
                self.model_struct.append(f"Tensor{dim}D_{dtype_str} {name};")

                for i in range(dim):
                    self.model_init.append(f"model->{name}.shape[{i}] = {tensor.shape[i]};")

                if initialized:
                    if tensor_dict["bypass"]:
                        continue
                    if qbit == 0:
                        self.model_init.append(f"model->{name}.data = (float *)(model_weight_data + {len(self.weight_content)});")    
                        self.weight_content += tensor.detach().numpy().tobytes()
                        # If align > 32, we need to align the weight data to the specified alignment
                        # Currently only consider 64-bit alignment
                        if len(self.weight_content) % (align_to // 8) != 0:
                            self.weight_content += b'\x00' * (len(self.weight_content) % (align_to // 8))
                    else:
                        qweight, scale = weight_quant(tensor, qbit)
                        self.model_init.append(f"model->{name}.data = (qbyte *)(model_weight_data + {len(self.weight_content)});")
                        self.weight_content += weight_export(qweight, qbit, align_to)
                        self.model_init.append(f"model->{name}.scale = {scale};")
                else:
                    n_size = tensor.nelement() * tensor.element_size()
                    self.model_init.append(f"model->{name}.data = (float *)malloc({n_size});")
            else:
                dim = 1
                dtype_str = "F32"
                self.model_struct.append(f"Tensor{dim}D_{dtype_str} {name};")
                self.model_init.append(f"model->{name}.shape[0] = 0;")

        print("finished tracing the model")

        # === Write the generated C code to the output directory. ===
        # create the output directory if it doesn't exist
        os.makedirs(output_directory, exist_ok=True)

        INDENT = "    "
        model_struct = [f"{INDENT}{line}" for line in self.model_struct]
        model_init = [f"{INDENT}{line}" for line in self.model_init]
        model_forward = [f"{INDENT}{line}" for line in self.model_forward]
        
        # Insert Profiler
        start_profiler = [f"{INDENT}long profile_time = MiCo_time();"]
        end_profiler = [f"{INDENT}profile_time = MiCo_time() - profile_time;",
                        f"{INDENT}printf(\"Execution Time: %ld\\n\", profile_time);",
                        f"{INDENT}printf(\"QMatMul Time: %ld\\n\", QMATMUL_TIMER);",
                        f"{INDENT}printf(\"Quantization Time: %ld\\n\", QUANT_TIMER);",
                        f"{INDENT}printf(\"Im2Col Time: %ld\\n\", IM2COL_TIMER);"]
        model_forward = start_profiler + model_forward + end_profiler

        model_struct_str = "\n".join(model_struct)
        model_init_str = "\n".join(model_init)
        if verbose:
            model_forward_str = ""
            func_count = 0
            for line in model_forward:
                model_forward_str += f"{INDENT}printf(\"{func_count}:{line[len(INDENT):line.find('(')]}\\n\");\n"
                model_forward_str += f"{line}\n"
                func_count += 1
        else:
            model_forward_str = "\n".join(model_forward)

        model_h_path = os.path.join(output_directory, f"{model_name}.h")
        model_bin_path = os.path.join(output_directory, f"{model_name}.bin")

        if hasattr(self.model, "default_dataset"):
            model_dataset = self.model.default_dataset
        else:
            model_dataset = "DEFAULT_DATASET"

        with open(model_h_path, "w") as f:
            f.write(MiCoCodeGen.MODEL_H_TEMPLATE.format(
                model_dataset=model_dataset,
                model_name=model_name,
                model_struct=model_struct_str,
                model_init=model_init_str,
                model_forward=model_forward_str
            ))
        
        with open(model_bin_path, "wb") as f:
            f.write(self.weight_content)
        
        print(f"wrote the model to {model_h_path} and {model_bin_path}")
        print(f"model size = {len(self.weight_content)} bytes")

    def build(self, build_dir: str = "project", target: str = "mico", options: str = ""):
        """
        Build the model using the provided build directory.
        """
        # check if the build directory exists
        if not os.path.exists(build_dir):
            raise FileNotFoundError(f"Build directory {build_dir} does not exist.")
        
        target_options = ""
        # compile the model
        if target == "mico":
            target_options += "TARGET=vexii OPT=simd MARCH=rv32imc"
        if target == "mico_fpu":
            target_options += "TARGET=vexii OPT=simd MARCH=rv32imfc"
        elif target == "vexii":
            target_options += "TARGET=vexii MARCH=rv32imc"
        elif target == "vexii_fpu":
            target_options += "TARGET=vexii MARCH=rv32imfc"
        elif target == "host":
            target_options += ""
        target_options += " " + options
        cmd = f"cd {build_dir} && make clean && make -j " + target_options
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        proc.wait()
        if proc.stderr:
            print("Error in compiling the model:")
            print(proc.stderr.readlines())
            return None
        else:
            print("Model compiled successfully!")
        return proc.returncode
    
    def extract_tensor_dag(self):
        """
        Extract the tensor dependencies from the graph as a Directed Acyclic Graph.
        
        Returns:
            dict: A dictionary where keys are node names and values are lists of nodes they depend on.
        """
        dag = {}
        
        # Iterate through all nodes in the graph
        for node in self.graph.nodes:
            if node.op == 'output':
                continue  # Skip output nodes for the dependency collection
            
            # For this node, find all input tensors it depends on
            dependencies = []
            
            if node.op in ['call_module', 'call_function', 'call_method']:
                # Get arguments to the call
                for arg in self.node_info[node.name][0]:
                    if isinstance(arg, torch.fx.node.Node):
                        dependencies.append(arg.name)
                    elif isinstance(arg, (list, tuple)) or hasattr(arg, '__iter__') and not isinstance(arg, str):
                        for item in arg:
                            if isinstance(item, torch.fx.node.Node):
                                dependencies.append(item.name)
            
            # Store the dependencies for this node
            dag[node.name] = dependencies
        
        return dag
    
    def node_is_bit_op(self, node):
        is_bit_op = False
        for n in self.graph.nodes:
            if n.name == node:
                if n.op == 'call_module':
                    module = self.get_module(n.target)
                    is_bit_op = isinstance(module, (BitConv2d, BitLinear))
                    break
        return is_bit_op
    
    def extract_simplified_dag(self):
        """
        Extract a simplified tensor DAG where non-quantized operations are aggregated
        while BitConv2d and BitLinear operations are preserved.
        
        Args:
            keep_bit_ops_only (bool): If True, only keep BitConv2d and BitLinear operations.
            
        Returns:
            dict: A simplified DAG dictionary.
        """
        # Get the full DAG first
        full_dag = self.extract_tensor_dag()
                
        # Create simplified DAG
        simplified_dag = {}

        for node, dependencies in full_dag.items():
            if self.node_is_bit_op(node):
                # If the node is a BitConv2d or BitLinear operation, keep it
                simplified_dag[node] = []
                deps = dependencies.copy()
                while len(deps) > 0:
                    dep = deps.pop(0)
                    if self.node_is_bit_op(dep):
                        simplified_dag[node].append(dep)
                    else:
                        deps += full_dag[dep]

        return simplified_dag
    
    def visualize_dag(self, output_file="model_dag.png", simplified=False):
        """
        Visualize the tensor dependency DAG using graphviz.
        
        Args:
            output_file (str): The output file to save the visualization.
            simplified (bool): If True, use the simplified DAG.
            bit_ops_only (bool): If True, only show BitConv2d and BitLinear operations.
            
        Returns:
            bool: True if successful, False otherwise.
        """
        try:
            import graphviz
        except ImportError:
            print("Graphviz not found. Please install graphviz with 'pip install graphviz'")
            return False
        
        # Get the appropriate DAG
        if simplified:
            dag = self.extract_simplified_dag()
        else:
            dag = self.extract_tensor_dag()
        
        # Create a new graph
        title = 'Model Tensor Dependencies'
        if simplified:
            title += ' (Simplified)'
            
        dot = graphviz.Digraph(comment=title)
        
        # Add nodes
        for node in dag:
            # Get node type if available
            node_type = "Unknown"
            node_bits = ""
            is_bit_op = False
            for n in self.graph.nodes:
                if n.name == node:
                    if n.op == 'call_module':
                        module = self.get_module(n.target)
                        node_type = type(module).__name__
                        is_bit_op = isinstance(module, (BitConv2d, BitLinear))
                    elif n.op == 'call_function':
                        node_type = n.target.__name__ if hasattr(n.target, '__name__') else str(n.target)
                    else:
                        node_type = n.op
                    break
            
            # Use different colors for BitConv2d/BitLinear nodes
            attrs = {}
            if is_bit_op:
                attrs['color'] = 'red'
                attrs['style'] = 'filled'
                attrs['fillcolor'] = 'lightpink'

                node_bits = f"W{module.qtype}A{module.act_q}"

            node_info = f"{node}\n({node_type})"
            if is_bit_op:
                node_info += f"({node_bits})"
            # Add node with type info
            dot.node(node, node_info, **attrs)
        
        # Add edges
        for node, dependencies in dag.items():
            for dep in dependencies:
                if dep in dag:  # Only add edge if both nodes are in the DAG
                    dot.edge(dep, node)
        
        # Render the graph
        try:
            dot.render(output_file.rsplit('.', 1)[0], format=output_file.rsplit('.', 1)[1], cleanup=True)
            print(f"DAG visualization saved to {output_file}")
            return True
        except Exception as e:
            print(f"Error rendering graph: {e}")
            return False

    
    def tensor_lifetime(self):

        dag = self.extract_tensor_dag()
        footprint = {}
        for node in dag:
            footprint[node] = 0
            for dep in dag[node]:
                tensor = self.tensors[dep]['tensor']
                footprint[node] += tensor.element_size() * tensor.nelement()
        print(footprint)
        return


    def forward(self, *args):
        self.reset()
        self.example_inputs = args

        output = self.run(*args)

        return output
    
    def __call__(self, *args):
        return self.forward(*args)

if __name__ == "__main__":
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import MiCoUtils as mico
    from models import MLP, LeNet, CmsisCNN, VGG, SqueezeNet, MobileNetV2, \
          resnet_alt_8, resnet_alt_18, shufflenet

    torch.manual_seed(0)

    # example_input = torch.randn(1, 256) # MNIST Flatten
    example_input = torch.randn(1, 1, 28, 28) # MNIST 28x28
    # example_input = torch.randn(1, 3, 32, 32) # CIFAR-10/100

    # m = MLP(in_features=256, config={"Layers": [64, 64, 64, 10]})
    # ckpt = torch.load("output/ckpt/mlp_mnist_mp.pth")

    # m = MLP(in_features=256, config={"Layers": [61, 53, 31, 10]})
    # ckpt = torch.load("output/ckpt/mlp_mnist_misalign.pth")

    m = LeNet(1)
    ckpt = torch.load("output/ckpt/lenet_mnist.pth")

    # m = CmsisCNN(in_channels=3)
    # ckpt = torch.load("output/ckpt/cmsiscnn_cifar10_mp.pth")

    # m = VGG(in_channels=3, num_class=10)
    # ckpt = torch.load("output/ckpt/vgg_cifar10.pth")

    # m = MobileNetV2(10)
    # ckpt = torch.load("output/ckpt/mobilenetv2_cifar10.pth")
    # m.default_dataset = "CIFAR10"

    # m = SqueezeNet(class_num=10)
    # ckpt = torch.load("output/ckpt/squeeze_cifar10.pth")
    # m.default_dataset = "CIFAR10"

    # m = shufflenet(10)
    # m.default_dataset = "CIFAR10"
    # ckpt = torch.load("output/ckpt/shuffle_cifar10.pth")

    # m = resnet_alt_8(10)
    # m.default_dataset = "CIFAR10"
    # ckpt = torch.load("output/ckpt/resnet8_cifar10.pth")

    # m = resnet_alt_18(100)
    # ckpt = torch.load("output/ckpt/resnet18_cifar100.pth", map_location="cpu")

    weight_q = [8] * m.n_layers
    activation_q = [8] * m.n_layers

    m.load_state_dict(ckpt)
    m.set_qscheme([weight_q, activation_q])
    m=fuse_model(m)
    m.eval()

    m = MiCoCodeGen(m, align_to=32)
    m.forward(example_input)
    # m.visualize_dag("model_full.png")
    # m.visualize_dag("model_simplified.png", simplified=True)
    m.convert("project", "model", verbose = True)
    # m.tensor_lifetime()
    # m.build("project", "mico_fpu", "TEST_NUM=1")
