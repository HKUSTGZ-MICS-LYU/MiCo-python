"""
MiCo MLIR Code Generator

This module provides MLIR code generation for MiCo mixed-precision quantized models.
It generates MLIR code using a custom MiCo dialect that supports sub-byte data types
and quantized neural network operations.

Example Usage:
    from models import LeNet
    from MiCoMLIRGen import MiCoMLIRGen
    from MiCoUtils import fuse_model
    import torch

    model = LeNet(1)
    model.set_qscheme([[8, 6, 6, 4, 4], [8, 8, 8, 8, 8]])
    model = fuse_model(model)
    model.eval()

    mlir_gen = MiCoMLIRGen(model)
    mlir_gen.forward(torch.randn(1, 1, 28, 28))
    mlir_gen.convert("output", "lenet_mnist")
"""

import os
import logging
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.fx

from MiCoCodeGen import MiCoCodeGen, MiCoTrace
from MiCoQLayers import BitLinear, BitConv1d, BitConv2d, BitQLayer, weight_quant
from MiCoRegistry import MiCoOpRegistry


class MiCoMLIRGen(MiCoCodeGen):
    """
    MLIR code generator for MiCo models.
    
    Generates MLIR code using the MiCo dialect for mixed-precision quantized models.
    The generated MLIR uses custom operations that represent sub-byte quantized
    neural network layers.
    
    Attributes:
        model: The PyTorch model to convert
        dialect: The MLIR dialect to use (default: "mico")
        mlir_ops: List of MLIR operations generated
        mlir_weights: Dictionary of weight tensors with metadata
        ssa_counter: Counter for SSA value naming
    """
    
    MLIR_TEMPLATE = """// MiCo MLIR Module - {model_name}
// Generated by MiCoMLIRGen
// This file uses the MiCo dialect for mixed-precision quantized neural networks

module @{model_name} {{

{weight_declarations}

{forward_function}

}}
"""
    
    def __init__(self, model: nn.Module, dialect: str = "mico", 
                 log_level: int = logging.INFO):
        """
        Initialize the MLIR code generator.
        
        Args:
            model: PyTorch model to convert
            dialect: MLIR dialect to use (default: "mico")
            log_level: Logging level
        """
        # Initialize parent class (handles graph extraction)
        super().__init__(model, align_to=1, log_level=log_level, gemmini_mode=False)
        
        self.dialect = dialect
        self.mlir_ops: List[str] = []
        self.mlir_weights: Dict[str, Dict[str, Any]] = {}
        self.mlir_weight_declarations: List[str] = []
        self.ssa_counter: int = 0
        self.ssa_mapping: Dict[str, str] = {}  # Maps node names to SSA values
        
        self.logger = logging.getLogger("MiCoMLIRGen")
        self.logger.setLevel(log_level)
    
    def reset(self):
        """Reset the generator state."""
        super().reset()
        self.mlir_ops = []
        self.mlir_weights = {}
        self.mlir_weight_declarations = []
        self.ssa_counter = 0
        self.ssa_mapping = {}
        self.example_inputs = None
    
    def _get_ssa_name(self, prefix: str = "v") -> str:
        """Generate a new SSA value name."""
        name = f"%{prefix}{self.ssa_counter}"
        self.ssa_counter += 1
        return name
    
    def _register_ssa(self, node_name: str, ssa_name: str):
        """Register a mapping from node name to SSA value."""
        self.ssa_mapping[node_name] = ssa_name
    
    def _get_ssa(self, node_name: str) -> str:
        """Get the SSA value for a node name."""
        return self.ssa_mapping.get(node_name, f"%{node_name}")
    
    def _format_tensor_type(self, tensor: torch.Tensor, qbit: int = 0) -> str:
        """
        Format a tensor type for MLIR.
        
        Args:
            tensor: The tensor to get type for
            qbit: Quantization bits (0 for full precision)
            
        Returns:
            MLIR type string (e.g., "tensor<1x64x28x28xf32>")
        """
        shape = "x".join(str(d) for d in tensor.shape)
        
        if qbit > 0:
            elem_type = f"!mico.int<{qbit}>"
        elif tensor.dtype == torch.float32:
            elem_type = "f32"
        elif tensor.dtype == torch.float16:
            elem_type = "f16"
        elif tensor.dtype == torch.int8:
            elem_type = "i8"
        elif tensor.dtype == torch.int32:
            elem_type = "i32"
        else:
            elem_type = "f32"
        
        return f"tensor<{shape}x{elem_type}>"
    
    def _format_shape(self, tensor: torch.Tensor) -> str:
        """Format tensor shape as MLIR dimension list."""
        return "x".join(str(d) for d in tensor.shape)
    
    def _format_list(self, values: List, prefix: str = "") -> str:
        """Format a list of values for MLIR attributes."""
        formatted = ", ".join(str(v) for v in values)
        return f"[{formatted}]"
    
    def _add_weight_constant(self, name: str, tensor: torch.Tensor, 
                             qbit: int = 0, scale: float = 0.0):
        """
        Add a weight constant declaration.
        
        Args:
            name: Name of the weight constant
            tensor: Weight tensor
            qbit: Quantization bits (0 for full precision)
            scale: Quantization scale factor
        """
        tensor_type = self._format_tensor_type(tensor, qbit)
        
        # Store weight metadata
        self.mlir_weights[name] = {
            "tensor": tensor,
            "type": tensor_type,
            "qbit": qbit,
            "scale": scale
        }
        
        # Generate constant declaration
        # For now, use opaque constant with placeholder
        if qbit > 0:
            decl = f'    mico.constant @{name} : {tensor_type} {{scale = {scale:.6f} : f32}}'
        else:
            decl = f'    mico.constant @{name} : {tensor_type}'
        
        self.mlir_weight_declarations.append(decl)
    
    def _add_mlir_op(self, op: str):
        """Add an MLIR operation to the list."""
        self.mlir_ops.append(f"        {op}")
    
    def handle_placeholder(self, n: torch.fx.node.Node, out: torch.Tensor):
        """Handle input placeholder nodes."""
        super().handle_placeholder(n, out)
        
        # Register input as function argument
        ssa_name = f"%{n.name}"
        self._register_ssa(n.name, ssa_name)
        
        self.logger.debug(f"MLIR placeholder: {n.name} -> {ssa_name}")
    
    def handle_call_module(self, n: torch.fx.node.Node, out: torch.Tensor):
        """Handle module calls (Conv2d, Linear, etc.)."""
        # Call parent for tensor tracking
        super().handle_call_module(n, out)
        
        module = self.get_module(n.target)
        input_node = self.node_info[n.name][0][0]
        input_ssa = self._get_ssa(input_node.name)
        result_ssa = self._get_ssa_name()
        result_type = self._format_tensor_type(out)
        
        self._register_ssa(n.name, result_ssa)
        
        if isinstance(module, BitLinear):
            self._handle_bitlinear(n, module, input_ssa, result_ssa, result_type, out)
        elif isinstance(module, BitConv2d):
            self._handle_bitconv2d(n, module, input_ssa, result_ssa, result_type, out)
        elif isinstance(module, BitConv1d):
            self._handle_bitconv1d(n, module, input_ssa, result_ssa, result_type, out)
        elif isinstance(module, nn.Linear):
            self._handle_linear(n, module, input_ssa, result_ssa, result_type, out)
        elif isinstance(module, nn.Conv2d):
            self._handle_conv2d(n, module, input_ssa, result_ssa, result_type, out)
        elif isinstance(module, nn.Conv1d):
            self._handle_conv1d(n, module, input_ssa, result_ssa, result_type, out)
        elif isinstance(module, nn.ReLU):
            self._handle_relu(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.ReLU6):
            self._handle_relu6(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.MaxPool2d):
            self._handle_maxpool2d(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.AvgPool2d):
            self._handle_avgpool2d(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.AdaptiveAvgPool2d):
            self._handle_adaptive_avgpool2d(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.Flatten):
            self._handle_flatten(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, (nn.Identity, nn.Dropout)):
            # Pass-through operations
            self._register_ssa(n.name, input_ssa)
        elif isinstance(module, nn.BatchNorm2d):
            self._handle_batchnorm2d(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.Tanh):
            self._handle_tanh(n, module, input_ssa, result_ssa, result_type)
        elif isinstance(module, nn.ELU):
            self._handle_elu(n, module, input_ssa, result_ssa, result_type)
        else:
            self.logger.warning(f"Unsupported module type: {type(module).__name__}")
            # Create a generic operation comment
            self._add_mlir_op(f"// Unsupported: {type(module).__name__}")
            self._register_ssa(n.name, input_ssa)
    
    def _handle_bitlinear(self, n, module: BitLinear, input_ssa: str, 
                          result_ssa: str, result_type: str, out: torch.Tensor):
        """Handle BitLinear quantized linear layer."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        
        # Add weight and bias constants
        self._add_weight_constant(weight_name, module.weight, 
                                  qbit=module.qtype, scale=float(module.qw_scale) if module.qw_scale else 1.0)
        if module.bias is not None:
            self._add_weight_constant(bias_name, module.bias)
        
        # Generate operation
        attrs = f"weight_bits = {module.qtype} : i32, act_bits = {module.act_q} : i32"
        input_type = self._format_tensor_type(out) if hasattr(out, 'shape') else result_type
        op = f"{result_ssa} = mico.bitlinear({input_ssa}, @{weight_name}, @{bias_name}) {{{attrs}}} : {input_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_bitconv2d(self, n, module: BitConv2d, input_ssa: str,
                          result_ssa: str, result_type: str, out: torch.Tensor):
        """Handle BitConv2d quantized 2D convolution."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        
        # Add weight and bias constants
        self._add_weight_constant(weight_name, module.weight,
                                  qbit=module.qtype, scale=float(module.qw_scale) if module.qw_scale else 1.0)
        if module.bias is not None:
            self._add_weight_constant(bias_name, module.bias)
        
        # Extract convolution parameters
        stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride, module.stride]
        padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding, module.padding]
        dilation = module.dilation if isinstance(module.dilation, (list, tuple)) else [module.dilation, module.dilation]
        
        # Generate operation
        attrs = (f"weight_bits = {module.qtype} : i32, act_bits = {module.act_q} : i32, "
                f"stride = {self._format_list(stride)}, padding = {self._format_list(padding)}, "
                f"dilation = {self._format_list(dilation)}, groups = {module.groups} : i32")
        input_type = self._format_tensor_type(out) if hasattr(out, 'shape') else result_type
        op = f"{result_ssa} = mico.bitconv2d({input_ssa}, @{weight_name}, @{bias_name}) {{{attrs}}} : {input_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_bitconv1d(self, n, module: BitConv1d, input_ssa: str,
                          result_ssa: str, result_type: str, out: torch.Tensor):
        """Handle BitConv1d quantized 1D convolution."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        
        # Add weight and bias constants
        self._add_weight_constant(weight_name, module.weight,
                                  qbit=module.qtype, scale=float(module.qw_scale) if module.qw_scale else 1.0)
        if module.bias is not None:
            self._add_weight_constant(bias_name, module.bias)
        
        # Extract convolution parameters
        stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride]
        padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding]
        dilation = module.dilation if isinstance(module.dilation, (list, tuple)) else [module.dilation]
        
        # Generate operation
        attrs = (f"weight_bits = {module.qtype} : i32, act_bits = {module.act_q} : i32, "
                f"stride = {self._format_list(stride)}, padding = {self._format_list(padding)}, "
                f"dilation = {self._format_list(dilation)}, groups = {module.groups} : i32")
        input_type = self._format_tensor_type(out) if hasattr(out, 'shape') else result_type
        op = f"{result_ssa} = mico.bitconv1d({input_ssa}, @{weight_name}, @{bias_name}) {{{attrs}}} : {input_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_linear(self, n, module: nn.Linear, input_ssa: str,
                       result_ssa: str, result_type: str, out: torch.Tensor):
        """Handle standard Linear layer."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        
        self._add_weight_constant(weight_name, module.weight)
        if module.bias is not None:
            self._add_weight_constant(bias_name, module.bias)
        
        input_type = self._format_tensor_type(out) if hasattr(out, 'shape') else result_type
        op = f"{result_ssa} = mico.linear({input_ssa}, @{weight_name}, @{bias_name}) : {input_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_conv2d(self, n, module: nn.Conv2d, input_ssa: str,
                       result_ssa: str, result_type: str, out: torch.Tensor):
        """Handle standard Conv2d layer."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        
        self._add_weight_constant(weight_name, module.weight)
        if module.bias is not None:
            self._add_weight_constant(bias_name, module.bias)
        
        stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride, module.stride]
        padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding, module.padding]
        dilation = module.dilation if isinstance(module.dilation, (list, tuple)) else [module.dilation, module.dilation]
        
        attrs = (f"stride = {self._format_list(stride)}, padding = {self._format_list(padding)}, "
                f"dilation = {self._format_list(dilation)}, groups = {module.groups} : i32")
        input_type = self._format_tensor_type(out) if hasattr(out, 'shape') else result_type
        op = f"{result_ssa} = mico.conv2d({input_ssa}, @{weight_name}, @{bias_name}) {{{attrs}}} : {input_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_conv1d(self, n, module: nn.Conv1d, input_ssa: str,
                       result_ssa: str, result_type: str, out: torch.Tensor):
        """Handle standard Conv1d layer."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        
        self._add_weight_constant(weight_name, module.weight)
        if module.bias is not None:
            self._add_weight_constant(bias_name, module.bias)
        
        stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride]
        padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding]
        dilation = module.dilation if isinstance(module.dilation, (list, tuple)) else [module.dilation]
        
        attrs = (f"stride = {self._format_list(stride)}, padding = {self._format_list(padding)}, "
                f"dilation = {self._format_list(dilation)}, groups = {module.groups} : i32")
        input_type = self._format_tensor_type(out) if hasattr(out, 'shape') else result_type
        op = f"{result_ssa} = mico.conv1d({input_ssa}, @{weight_name}, @{bias_name}) {{{attrs}}} : {input_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_relu(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle ReLU activation."""
        op = f"{result_ssa} = mico.relu({input_ssa}) : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_relu6(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle ReLU6 activation."""
        op = f"{result_ssa} = mico.relu6({input_ssa}) : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_tanh(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle Tanh activation."""
        op = f"{result_ssa} = mico.tanh({input_ssa}) : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_elu(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle ELU activation."""
        op = f"{result_ssa} = mico.elu({input_ssa}) {{alpha = {module.alpha} : f32}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_maxpool2d(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle MaxPool2d."""
        kernel_size = module.kernel_size if isinstance(module.kernel_size, (list, tuple)) else [module.kernel_size, module.kernel_size]
        stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride, module.stride]
        padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding, module.padding]
        
        attrs = f"kernel_size = {self._format_list(kernel_size)}, stride = {self._format_list(stride)}, padding = {self._format_list(padding)}"
        op = f"{result_ssa} = mico.maxpool2d({input_ssa}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_avgpool2d(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle AvgPool2d."""
        kernel_size = module.kernel_size if isinstance(module.kernel_size, (list, tuple)) else [module.kernel_size, module.kernel_size]
        stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride, module.stride]
        padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding, module.padding]
        
        attrs = f"kernel_size = {self._format_list(kernel_size)}, stride = {self._format_list(stride)}, padding = {self._format_list(padding)}"
        op = f"{result_ssa} = mico.avgpool2d({input_ssa}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_adaptive_avgpool2d(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle AdaptiveAvgPool2d."""
        output_size = module.output_size if isinstance(module.output_size, (list, tuple)) else [module.output_size, module.output_size]
        
        attrs = f"output_size = {self._format_list(output_size)}"
        op = f"{result_ssa} = mico.adaptive_avgpool2d({input_ssa}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_flatten(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle Flatten operation."""
        start_dim = module.start_dim if hasattr(module, 'start_dim') else 1
        op = f"{result_ssa} = mico.flatten({input_ssa}) {{start_dim = {start_dim} : i32}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_batchnorm2d(self, n, module, input_ssa: str, result_ssa: str, result_type: str):
        """Handle BatchNorm2d."""
        weight_name = f"{n.name}_weight"
        bias_name = f"{n.name}_bias"
        mean_name = f"{n.name}_running_mean"
        var_name = f"{n.name}_running_var"
        
        self._add_weight_constant(weight_name, module.weight)
        self._add_weight_constant(bias_name, module.bias)
        self._add_weight_constant(mean_name, module.running_mean)
        self._add_weight_constant(var_name, module.running_var)
        
        attrs = f"eps = {module.eps} : f32"
        op = f"{result_ssa} = mico.batchnorm2d({input_ssa}, @{weight_name}, @{bias_name}, @{mean_name}, @{var_name}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def handle_call_function(self, n: torch.fx.node.Node, out: torch.Tensor):
        """Handle function calls (torch.add, F.relu, etc.)."""
        super().handle_call_function(n, out)
        
        function = n.target
        input_names = self._extract_input_names(n)
        input_args = n.args
        
        result_ssa = self._get_ssa_name()
        result_type = self._format_tensor_type(out)
        self._register_ssa(n.name, result_ssa)
        
        if function in (torch.add, __import__('operator').__add__):
            self._handle_add_function(n, input_names, result_ssa, result_type)
        elif function in (torch.nn.functional.relu,):
            input_ssa = self._get_ssa(input_names[0])
            op = f"{result_ssa} = mico.relu({input_ssa}) : {result_type} -> {result_type}"
            self._add_mlir_op(op)
        elif function in (torch.nn.functional.relu6,):
            input_ssa = self._get_ssa(input_names[0])
            op = f"{result_ssa} = mico.relu6({input_ssa}) : {result_type} -> {result_type}"
            self._add_mlir_op(op)
        elif function in (torch.flatten,):
            input_ssa = self._get_ssa(input_names[0])
            start_dim = input_args[1] if len(input_args) > 1 else 1
            op = f"{result_ssa} = mico.flatten({input_ssa}) {{start_dim = {start_dim} : i32}} : {result_type}"
            self._add_mlir_op(op)
        elif function in (torch.cat,):
            input_ssas = [self._get_ssa(name) for name in input_names]
            dim = input_args[1] if len(input_args) > 1 else 0
            inputs_str = ", ".join(input_ssas)
            op = f"{result_ssa} = mico.concat({inputs_str}) {{dim = {dim} : i32}} : {result_type}"
            self._add_mlir_op(op)
        elif function in (torch.nn.functional.max_pool2d,):
            self._handle_maxpool2d_function(n, input_names, input_args, result_ssa, result_type)
        elif function in (torch.nn.functional.avg_pool2d,):
            self._handle_avgpool2d_function(n, input_names, input_args, result_ssa, result_type)
        elif function in (torch.nn.functional.adaptive_avg_pool2d,):
            self._handle_adaptive_avgpool2d_function(n, input_names, input_args, result_ssa, result_type)
        else:
            # Fallback: log warning and pass through
            func_name = getattr(function, '__name__', str(function))
            self.logger.warning(f"Unsupported function: {func_name}")
            if input_names:
                self._register_ssa(n.name, self._get_ssa(input_names[0]))
            self._add_mlir_op(f"// Unsupported function: {func_name}")
    
    def _handle_add_function(self, n, input_names, result_ssa, result_type):
        """Handle element-wise addition."""
        lhs_ssa = self._get_ssa(input_names[0])
        rhs_ssa = self._get_ssa(input_names[1]) if len(input_names) > 1 else lhs_ssa
        op = f"{result_ssa} = mico.add({lhs_ssa}, {rhs_ssa}) : {result_type}, {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_maxpool2d_function(self, n, input_names, input_args, result_ssa, result_type):
        """Handle max_pool2d function."""
        input_ssa = self._get_ssa(input_names[0])
        kernel_size = input_args[1] if len(input_args) > 1 else [2, 2]
        stride = input_args[2] if len(input_args) > 2 else kernel_size
        padding = input_args[3] if len(input_args) > 3 else [0, 0]
        
        if isinstance(kernel_size, int):
            kernel_size = [kernel_size, kernel_size]
        if isinstance(stride, int):
            stride = [stride, stride]
        if isinstance(padding, int):
            padding = [padding, padding]
        
        attrs = f"kernel_size = {self._format_list(kernel_size)}, stride = {self._format_list(stride)}, padding = {self._format_list(padding)}"
        op = f"{result_ssa} = mico.maxpool2d({input_ssa}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_avgpool2d_function(self, n, input_names, input_args, result_ssa, result_type):
        """Handle avg_pool2d function."""
        input_ssa = self._get_ssa(input_names[0])
        kernel_size = input_args[1] if len(input_args) > 1 else [2, 2]
        stride = input_args[2] if len(input_args) > 2 else kernel_size
        padding = input_args[3] if len(input_args) > 3 else [0, 0]
        
        if isinstance(kernel_size, int):
            kernel_size = [kernel_size, kernel_size]
        if isinstance(stride, int):
            stride = [stride, stride]
        if isinstance(padding, int):
            padding = [padding, padding]
        
        attrs = f"kernel_size = {self._format_list(kernel_size)}, stride = {self._format_list(stride)}, padding = {self._format_list(padding)}"
        op = f"{result_ssa} = mico.avgpool2d({input_ssa}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def _handle_adaptive_avgpool2d_function(self, n, input_names, input_args, result_ssa, result_type):
        """Handle adaptive_avg_pool2d function."""
        input_ssa = self._get_ssa(input_names[0])
        output_size = input_args[1] if len(input_args) > 1 else [1, 1]
        
        if isinstance(output_size, int):
            output_size = [output_size, output_size]
        
        attrs = f"output_size = {self._format_list(output_size)}"
        op = f"{result_ssa} = mico.adaptive_avgpool2d({input_ssa}) {{{attrs}}} : {result_type} -> {result_type}"
        self._add_mlir_op(op)
    
    def handle_output(self, n: torch.fx.node.Node, out: torch.Tensor):
        """Handle output node."""
        super().handle_output(n, out)
        
        # Get the input to the output node
        if n.args and hasattr(n.args[0], 'name'):
            output_ssa = self._get_ssa(n.args[0].name)
            result_type = self._format_tensor_type(out)
            self._add_mlir_op(f"return {output_ssa} : {result_type}")
    
    def forward(self, *args):
        """
        Run forward pass to trace the model and collect MLIR operations.
        
        Args:
            *args: Input tensors
            
        Returns:
            Output tensor from the model
        """
        self.reset()
        self.example_inputs = args
        return self.run(*args)
    
    def convert(self, output_directory: str = "output", 
                model_name: str = "model",
                verbose: bool = False) -> str:
        """
        Convert the traced model to MLIR code.
        
        Args:
            output_directory: Directory to write output files
            model_name: Name for the generated model
            verbose: Enable verbose output
            
        Returns:
            Path to the generated MLIR file
        """
        if self.example_inputs is None:
            raise ValueError("No example inputs provided. Please call forward() first.")
        
        # Create output directory
        os.makedirs(output_directory, exist_ok=True)
        
        # Determine input type for function signature
        if len(self.example_inputs) > 0:
            input_tensor = self.example_inputs[0]
            input_type = self._format_tensor_type(input_tensor)
            input_name = "input"
        else:
            input_type = "tensor<1x1xf32>"
            input_name = "input"
        
        # Get output type from last operation
        output_type = "tensor<1x10xf32>"  # Default
        for op in reversed(self.mlir_ops):
            if "return" in op:
                # Extract return type
                parts = op.split(":")
                if len(parts) > 1:
                    output_type = parts[-1].strip()
                break
        
        # Build forward function
        func_signature = f"    func.func @forward(%{input_name}: {input_type}) -> {output_type} {{"
        operations = "\n".join(self.mlir_ops)
        func_end = "    }"
        forward_function = f"{func_signature}\n{operations}\n{func_end}"
        
        # Build weight declarations
        weight_declarations = "\n".join(self.mlir_weight_declarations)
        
        # Generate complete MLIR code
        mlir_code = MiCoMLIRGen.MLIR_TEMPLATE.format(
            model_name=model_name,
            weight_declarations=weight_declarations,
            forward_function=forward_function
        )
        
        # Write to file
        mlir_path = os.path.join(output_directory, f"{model_name}.mlir")
        with open(mlir_path, "w") as f:
            f.write(mlir_code)
        
        if verbose:
            print(f"Generated MLIR code:\n{mlir_code}")
        
        print(f"MLIR code written to: {mlir_path}")
        print(f"Number of operations: {len(self.mlir_ops)}")
        print(f"Number of weight constants: {len(self.mlir_weights)}")
        
        return mlir_path
    
    def get_mlir_code(self) -> str:
        """
        Get the generated MLIR code as a string without writing to file.
        
        Returns:
            MLIR code as a string
        """
        if len(self.mlir_ops) == 0:
            raise ValueError("No operations generated. Please call forward() first.")
        
        # Simplified version for inspection
        weight_declarations = "\n".join(self.mlir_weight_declarations)
        operations = "\n".join(self.mlir_ops)
        
        return f"// Weight declarations:\n{weight_declarations}\n\n// Operations:\n{operations}"


if __name__ == "__main__":
    # Example usage
    import sys
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    from models import MLP, LeNet
    from MiCoUtils import fuse_model
    
    print("=" * 60)
    print("MiCo MLIR Code Generator Example")
    print("=" * 60)
    
    # Example 1: MLP
    print("\n--- Example 1: MLP ---")
    mlp = MLP(in_features=64, config={"Layers": [32, 16, 10]})
    weight_q = [8, 6, 4]  # Mixed precision
    activation_q = [8, 8, 8]
    mlp.set_qscheme([weight_q, activation_q])
    mlp = fuse_model(mlp)
    mlp.eval()
    
    mlir_gen = MiCoMLIRGen(mlp)
    mlir_gen.forward(torch.randn(1, 64))
    mlir_path = mlir_gen.convert("output", "mlp_mnist_mlir", verbose=False)
    
    print(f"\nGenerated MLIR at: {mlir_path}")
    print("\nMLIR Code Preview:")
    print(mlir_gen.get_mlir_code())
    
    # Example 2: LeNet
    print("\n--- Example 2: LeNet ---")
    lenet = LeNet(1)
    weight_q = [8, 6, 6, 4, 4]  # Mixed precision for conv1, conv2, fc1, fc2, fc3
    activation_q = [8, 8, 8, 8, 8]
    lenet.set_qscheme([weight_q, activation_q])
    lenet = fuse_model(lenet)
    lenet.eval()
    
    mlir_gen = MiCoMLIRGen(lenet)
    mlir_gen.forward(torch.randn(1, 1, 28, 28))
    mlir_path = mlir_gen.convert("output", "lenet_mnist_mlir", verbose=False)
    
    print(f"\nGenerated MLIR at: {mlir_path}")
    
    print("\n" + "=" * 60)
    print("MLIR Code Generation Complete!")
    print("=" * 60)
