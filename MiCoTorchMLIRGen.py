"""
MiCo Torch-MLIR Code Generator

This module provides MLIR code generation for MiCo mixed-precision quantized models
using torch-mlir as the first pass for PyTorch to MLIR conversion, followed by
custom MiCo dialect lowering for sub-byte quantization support.

The pipeline is:
    PyTorch Model → torch-mlir (Torch dialect) → MiCo dialect (sub-byte types)

Example Usage:
    from models import LeNet
    from MiCoTorchMLIRGen import MiCoTorchMLIRGen
    from MiCoUtils import fuse_model
    import torch

    model = LeNet(1)
    model.set_qscheme([[8, 6, 6, 4, 4], [8, 8, 8, 8, 8]])
    model = fuse_model(model)
    model.eval()

    # Use torch-mlir backend
    mlir_gen = MiCoTorchMLIRGen(model)
    mlir_gen.forward(torch.randn(1, 1, 28, 28))
    mlir_gen.convert("output", "lenet_mnist")
"""

import os
import logging
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.fx

# Check if torch-mlir is available
TORCH_MLIR_AVAILABLE = False
try:
    import torch_mlir
    from torch_mlir import torchscript
    TORCH_MLIR_AVAILABLE = True
except ImportError:
    pass

from MiCoCodeGen import MiCoCodeGen, MiCoTrace
from MiCoMLIRGen import MiCoMLIRGen
from MiCoQLayers import BitLinear, BitConv1d, BitConv2d, BitQLayer, weight_quant


class MiCoTorchMLIRGen(MiCoMLIRGen):
    """
    MLIR code generator using torch-mlir as the first pass.
    
    This generator uses torch-mlir to convert PyTorch models to MLIR's Torch dialect,
    then applies custom lowering to the MiCo dialect for sub-byte quantization.
    
    If torch-mlir is not available, falls back to the standalone MiCoMLIRGen.
    
    Attributes:
        model: The PyTorch model to convert
        use_torch_mlir: Whether to use torch-mlir (True if available)
        output_type: The torch-mlir output type ("torch", "linalg", "stablehlo")
    """
    
    TORCH_MLIR_TEMPLATE = """// MiCo MLIR Module - {model_name}
// Generated by MiCoTorchMLIRGen with torch-mlir backend
// Pipeline: PyTorch → torch-mlir (Torch dialect) → MiCo dialect

// ============================================================================
// Torch-MLIR Generated IR (Torch Dialect)
// ============================================================================
{torch_mlir_ir}

// ============================================================================
// MiCo Dialect Overlay (Sub-byte Quantization)
// ============================================================================
// The following module contains MiCo-specific quantization metadata that
// should be used in conjunction with the Torch dialect IR above.

module @{model_name}_mico {{

{weight_declarations}

{mico_ops}

}}
"""

    MICO_STANDALONE_TEMPLATE = """// MiCo MLIR Module - {model_name}
// Generated by MiCoTorchMLIRGen (standalone mode - torch-mlir not available)
// This file uses the MiCo dialect for mixed-precision quantized neural networks

module @{model_name} {{

{weight_declarations}

{forward_function}

}}
"""
    
    def __init__(self, model: nn.Module, dialect: str = "mico",
                 output_type: str = "torch", log_level: int = logging.INFO):
        """
        Initialize the Torch-MLIR code generator.
        
        Args:
            model: PyTorch model to convert
            dialect: MLIR dialect to use for MiCo ops (default: "mico")
            output_type: torch-mlir output type: "torch", "linalg", or "stablehlo"
            log_level: Logging level
        """
        super().__init__(model, dialect=dialect, log_level=log_level)
        
        self.use_torch_mlir = TORCH_MLIR_AVAILABLE
        self.output_type = output_type
        self.torch_mlir_ir: Optional[str] = None
        self.mico_quantization_ops: List[str] = []
        
        self.logger = logging.getLogger("MiCoTorchMLIRGen")
        self.logger.setLevel(log_level)
        
        if not TORCH_MLIR_AVAILABLE:
            self.logger.warning(
                "torch-mlir not available. Install with: "
                "pip install torch-mlir -f https://github.com/llvm/torch-mlir-release/releases"
            )
            self.logger.info("Falling back to standalone MiCoMLIRGen mode")
    
    def reset(self):
        """Reset the generator state."""
        super().reset()
        self.torch_mlir_ir = None
        self.mico_quantization_ops = []
    
    def _convert_with_torch_mlir(self, example_inputs) -> str:
        """
        Convert the model using torch-mlir.
        
        Args:
            example_inputs: Example input tensors for tracing (tuple or single tensor)
            
        Returns:
            MLIR string in Torch dialect
        """
        if not TORCH_MLIR_AVAILABLE:
            raise RuntimeError("torch-mlir is not available")
        
        # Get the output type enum
        if self.output_type == "torch":
            output_type = torchscript.OutputType.TORCH
        elif self.output_type == "linalg":
            output_type = torchscript.OutputType.LINALG_ON_TENSORS
        elif self.output_type == "stablehlo":
            output_type = torchscript.OutputType.STABLEHLO
        else:
            output_type = torchscript.OutputType.TORCH
        
        # Create a wrapper model that doesn't have MiCo-specific quantization
        # torch-mlir expects standard PyTorch operations
        class ModelWrapper(nn.Module):
            def __init__(self, original_model):
                super().__init__()
                self.model = original_model
            
            def forward(self, x):
                return self.model(x)
        
        wrapped_model = ModelWrapper(self.model)
        wrapped_model.eval()
        
        try:
            # Compile with torch-mlir
            module = torchscript.compile(
                wrapped_model,
                example_inputs,
                output_type=output_type,
                use_tracing=True
            )
            
            # Get the MLIR string representation
            mlir_str = str(module)
            return mlir_str
            
        except Exception as e:
            self.logger.warning(f"torch-mlir compilation failed: {e}")
            self.logger.info("Falling back to standalone mode")
            self.use_torch_mlir = False
            return ""
    
    def _generate_mico_quantization_overlay(self) -> str:
        """
        Generate MiCo dialect operations for quantization metadata.
        
        This creates MiCo-specific operations that annotate the torch-mlir
        generated IR with sub-byte quantization information.
        
        Returns:
            MLIR string with MiCo quantization operations
        """
        ops = []
        
        # Add comments explaining the quantization scheme
        ops.append("    // Quantization scheme for mixed-precision inference")
        ops.append("    // Each layer has specific weight_bits and activation_bits")
        ops.append("")
        
        # Generate MiCo quantization ops based on the model's QLayers
        layer_idx = 0
        for name, module in self.model.named_modules():
            if isinstance(module, BitLinear):
                safe_name = name.replace(".", "_")
                ops.append(f"    // Layer: {name}")
                ops.append(f"    // mico.quantize_linear @{safe_name} {{")
                ops.append(f"    //     weight_bits = {module.qtype} : i32,")
                ops.append(f"    //     act_bits = {module.act_q} : i32,")
                if module.qw_scale is not None:
                    ops.append(f"    //     scale = {float(module.qw_scale):.6f} : f32")
                ops.append(f"    // }}")
                ops.append("")
                layer_idx += 1
                
            elif isinstance(module, BitConv2d):
                safe_name = name.replace(".", "_")
                stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride, module.stride]
                padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding, module.padding]
                
                ops.append(f"    // Layer: {name}")
                ops.append(f"    // mico.quantize_conv2d @{safe_name} {{")
                ops.append(f"    //     weight_bits = {module.qtype} : i32,")
                ops.append(f"    //     act_bits = {module.act_q} : i32,")
                ops.append(f"    //     stride = [{stride[0]}, {stride[1]}],")
                ops.append(f"    //     padding = [{padding[0]}, {padding[1]}],")
                if module.qw_scale is not None:
                    ops.append(f"    //     scale = {float(module.qw_scale):.6f} : f32")
                ops.append(f"    // }}")
                ops.append("")
                layer_idx += 1
                
            elif isinstance(module, BitConv1d):
                safe_name = name.replace(".", "_")
                stride = module.stride if isinstance(module.stride, (list, tuple)) else [module.stride]
                padding = module.padding if isinstance(module.padding, (list, tuple)) else [module.padding]
                
                ops.append(f"    // Layer: {name}")
                ops.append(f"    // mico.quantize_conv1d @{safe_name} {{")
                ops.append(f"    //     weight_bits = {module.qtype} : i32,")
                ops.append(f"    //     act_bits = {module.act_q} : i32,")
                ops.append(f"    //     stride = [{stride[0]}],")
                ops.append(f"    //     padding = [{padding[0]}],")
                if module.qw_scale is not None:
                    ops.append(f"    //     scale = {float(module.qw_scale):.6f} : f32")
                ops.append(f"    // }}")
                ops.append("")
                layer_idx += 1
        
        return "\n".join(ops)
    
    def _generate_mico_type_definitions(self) -> str:
        """
        Generate MiCo sub-byte type definitions.
        
        Returns:
            MLIR string with MiCo type definitions
        """
        types = []
        types.append("    // MiCo Sub-byte Integer Types")
        types.append("    // !mico.int<N> where N ∈ {1, 2, 4, 8}")
        types.append("")
        
        # Collect all unique bit widths used in the model
        bit_widths = set()
        for name, module in self.model.named_modules():
            if isinstance(module, (BitLinear, BitConv1d, BitConv2d)):
                bit_widths.add(module.qtype)
                bit_widths.add(module.act_q)
        
        for bw in sorted(bit_widths):
            types.append(f"    // !mico.int<{bw}> - {bw}-bit quantized integer")
        
        return "\n".join(types)
    
    def convert(self, output_directory: str = "output",
                model_name: str = "model",
                verbose: bool = False) -> str:
        """
        Convert the traced model to MLIR code.
        
        Args:
            output_directory: Directory to write output files
            model_name: Name for the generated model
            verbose: Enable verbose output
            
        Returns:
            Path to the generated MLIR file
        """
        if self.example_inputs is None:
            raise ValueError("No example inputs provided. Please call forward() first.")
        
        # Create output directory
        os.makedirs(output_directory, exist_ok=True)
        
        if self.use_torch_mlir:
            # Use torch-mlir for conversion
            self.logger.info("Using torch-mlir backend for MLIR generation")
            
            try:
                # Convert with torch-mlir
                self.torch_mlir_ir = self._convert_with_torch_mlir(self.example_inputs)
                
                if self.torch_mlir_ir:
                    # Generate MiCo quantization overlay
                    mico_type_defs = self._generate_mico_type_definitions()
                    mico_quant_ops = self._generate_mico_quantization_overlay()
                    
                    # Combine weight declarations
                    weight_declarations = "\n".join(self.mlir_weight_declarations) if self.mlir_weight_declarations else "    // No weight constants (weights embedded in torch-mlir IR)"
                    
                    # Generate combined output
                    mlir_code = self.TORCH_MLIR_TEMPLATE.format(
                        model_name=model_name,
                        torch_mlir_ir=self.torch_mlir_ir,
                        weight_declarations=mico_type_defs + "\n\n" + weight_declarations,
                        mico_ops=mico_quant_ops
                    )
                else:
                    # Fallback to standalone mode
                    self.use_torch_mlir = False
                    
            except Exception as e:
                self.logger.warning(f"torch-mlir conversion failed: {e}")
                self.use_torch_mlir = False
        
        if not self.use_torch_mlir:
            # Use standalone MiCoMLIRGen
            self.logger.info("Using standalone MiCoMLIRGen mode")
            return super().convert(output_directory, model_name, verbose)
        
        # Write to file
        mlir_path = os.path.join(output_directory, f"{model_name}_torch_mlir.mlir")
        with open(mlir_path, "w") as f:
            f.write(mlir_code)
        
        if verbose:
            print(f"Generated MLIR code:\n{mlir_code}")
        
        print(f"MLIR code written to: {mlir_path}")
        print(f"Backend: torch-mlir ({self.output_type})")
        
        return mlir_path
    
    def get_torch_mlir_ir(self) -> Optional[str]:
        """
        Get the torch-mlir generated IR.
        
        Returns:
            The torch-mlir MLIR string, or None if not available
        """
        return self.torch_mlir_ir
    
    def is_torch_mlir_available(self) -> bool:
        """
        Check if torch-mlir is available.
        
        Returns:
            True if torch-mlir is available and can be used
        """
        return TORCH_MLIR_AVAILABLE


def check_torch_mlir_installation() -> Dict[str, Any]:
    """
    Check torch-mlir installation status.
    
    Returns:
        Dictionary with installation status and version info
    """
    result = {
        "available": TORCH_MLIR_AVAILABLE,
        "version": None,
        "output_types": [],
        "install_command": "pip install torch-mlir -f https://github.com/llvm/torch-mlir-release/releases"
    }
    
    if TORCH_MLIR_AVAILABLE:
        try:
            result["version"] = torch_mlir.__version__
        except AttributeError:
            result["version"] = "unknown"
        
        result["output_types"] = ["torch", "linalg", "stablehlo"]
    
    return result


if __name__ == "__main__":
    # Example usage and installation check
    import sys
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    print("=" * 60)
    print("MiCo Torch-MLIR Code Generator")
    print("=" * 60)
    
    # Check installation
    status = check_torch_mlir_installation()
    print(f"\ntorch-mlir available: {status['available']}")
    if status['available']:
        print(f"torch-mlir version: {status['version']}")
        print(f"Supported output types: {status['output_types']}")
    else:
        print(f"Install with: {status['install_command']}")
    
    # Try to run an example
    print("\n" + "=" * 60)
    print("Running Example")
    print("=" * 60)
    
    try:
        from models import MLP, LeNet
        from MiCoUtils import fuse_model
        
        # Create a simple model
        model = MLP(in_features=64, config={"Layers": [32, 16, 10]})
        weight_q = [8, 6, 4]  # Mixed precision
        activation_q = [8, 8, 8]
        model.set_qscheme([weight_q, activation_q])
        model = fuse_model(model)
        model.eval()
        
        # Generate MLIR
        mlir_gen = MiCoTorchMLIRGen(model, output_type="torch")
        mlir_gen.forward(torch.randn(1, 64))
        mlir_path = mlir_gen.convert("output", "mlp_torch_mlir", verbose=False)
        
        print(f"\nGenerated MLIR at: {mlir_path}")
        print(f"Backend used: {'torch-mlir' if mlir_gen.use_torch_mlir else 'standalone'}")
        
    except Exception as e:
        print(f"Example failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("Done!")
    print("=" * 60)
